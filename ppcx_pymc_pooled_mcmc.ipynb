{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e564560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import arviz as az\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from src.clustering import preproc_features\n",
    "from src.config import ConfigManager\n",
    "from src.database import (\n",
    "    get_dic_analysis_by_ids,\n",
    "    get_dic_analysis_ids,\n",
    "    get_dic_data,\n",
    "    get_image,\n",
    "    get_multi_dic_data,\n",
    ")\n",
    "from src.preprocessing import apply_dic_filters, spatial_subsample\n",
    "from src.roi import PolygonROISelector\n",
    "\n",
    "%matplotlib widget\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "\n",
    "RANDOM_SEED = 8927\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "# Load configuration\n",
    "config = ConfigManager()\n",
    "db_engine = create_engine(config.db_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4863a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_date = \"2024-08-23\"\n",
    "camera_name = \"PPCX_Tele\"\n",
    "output_dir = Path(\"output\") / f\"{camera_name}_PyMC\"\n",
    "base_name = f\"{camera_name}_{target_date}_PyMC_GMM\"\n",
    "\n",
    "SUBSAMPLE_FACTOR = 2  # Take every n point\n",
    "SUBSAMPLE_METHOD = \"regular\"  # or 'random', 'stratified'\n",
    "PRIOR_STRENGTH = 0.4  # Between 0 and 1\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# dic_ids = (629,)\n",
    "# Get DIC analysis metadata\n",
    "# dic_ids = get_dic_analysis_ids(\n",
    "#     db_engine, reference_date=target_date, camera_name=camera_name\n",
    "# )\n",
    "\n",
    "# Get DIC analysis metadata\n",
    "reference_start_date = \"2024-08-23\"\n",
    "reference_end_date = \"2024-08-28\"\n",
    "dic_ids = get_dic_analysis_ids(\n",
    "    db_engine,\n",
    "    camera_name=camera_name,\n",
    "    reference_date_start=reference_start_date,\n",
    "    reference_date_end=reference_end_date,\n",
    ")\n",
    "\n",
    "dic_analyses = get_dic_analysis_by_ids(db_engine=db_engine, dic_ids=dic_ids)\n",
    "\n",
    "# Get master image\n",
    "master_image_id = dic_analyses[\"master_image_id\"].iloc[0]\n",
    "img = get_image(master_image_id, camera_name=camera_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bde16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch DIC data\n",
    "if len(dic_ids) == 0:\n",
    "    raise ValueError(\"No DIC analyses found for the given criteria\")\n",
    "elif len(dic_ids) == 1:\n",
    "    logging.info(f\"Using DIC ID: {dic_ids[0]}\")\n",
    "    df = get_dic_data(dic_ids[0])\n",
    "    df = apply_dic_filters(\n",
    "        df,\n",
    "        filter_outliers=config.get(\"dic.filter_outliers\"),\n",
    "        tails_percentile=config.get(\"dic.tails_percentile\"),\n",
    "        min_velocity=config.get(\"dic.min_velocity\"),\n",
    "        apply_2d_median=config.get(\"dic.apply_2d_median\"),\n",
    "        median_window_size=config.get(\"dic.median_window_size\"),\n",
    "        median_threshold_factor=config.get(\"dic.median_threshold_factor\"),\n",
    "    )\n",
    "else:\n",
    "    out = get_multi_dic_data(dic_ids, stack_results=False)\n",
    "    # Apply filter for each df in the dictionary and then stack them\n",
    "    logging.info(f\"Found stack of {len(out)} DIC dataframes. Run filtering...\")\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            apply_dic_filters(\n",
    "                df,\n",
    "                filter_outliers=config.get(\"dic.filter_outliers\"),\n",
    "                tails_percentile=config.get(\"dic.tails_percentile\"),\n",
    "                min_velocity=config.get(\"dic.min_velocity\"),\n",
    "                apply_2d_median=config.get(\"dic.apply_2d_median\"),\n",
    "                median_window_size=config.get(\"dic.median_window_size\"),\n",
    "                median_threshold_factor=config.get(\"dic.median_threshold_factor\"),\n",
    "            )\n",
    "            for df in out.values()\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "# To interactively select a ROI:\n",
    "# selector = PolygonROISelector(file_path=config.get(\"data.roi_path\"))\n",
    "# selector.draw_interactive(image=img)\n",
    "\n",
    "# Apply ROI filter\n",
    "selector = PolygonROISelector.from_file(config.get(\"data.roi_path\"))\n",
    "df = selector.filter_dataframe(df, x_col=\"x\", y_col=\"y\")\n",
    "logging.info(f\"Data shape after filtering: {df.shape}\")\n",
    "\n",
    "\n",
    "# Apply subsampling AFTER ROI filtering\n",
    "if SUBSAMPLE_FACTOR > 0:\n",
    "    logging.info(f\"Data shape before subsampling: {df.shape}\")\n",
    "    df_subsampled = spatial_subsample(\n",
    "        df, n_subsample=SUBSAMPLE_FACTOR, method=SUBSAMPLE_METHOD\n",
    "    )\n",
    "    df = df_subsampled\n",
    "    logging.info(f\"Data shape after subsampling: {df.shape}\")\n",
    "\n",
    "# === FEATURE PREPARATION ===\n",
    "\n",
    "# Get clustering parameters from config\n",
    "variables_names = config.get(\"clustering.variables_names\")\n",
    "logging.info(f\"Using features: {variables_names}\")\n",
    "\n",
    "# Preprocess features\n",
    "df_features = preproc_features(df)\n",
    "X = df_features[variables_names].values\n",
    "n_features = X.shape[1]\n",
    "ndata = X.shape[0]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "logging.info(f\"Feature matrix shape: {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1898a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SPATIAL PRIOR SETUP ===\n",
    "def assign_spatial_priors_dic(df, selectors, prior_strength=0.8):\n",
    "    \"\"\"Assign spatial prior probabilities based on polygon sectors.\"\"\"\n",
    "    ndata = len(df)\n",
    "    k = len(selectors)\n",
    "    prior_probs = np.ones((ndata, k)) / k  # default uniform\n",
    "\n",
    "    for idx, selector in enumerate(selectors):\n",
    "        mask = selector.contains_points(df[\"x\"].values, df[\"y\"].values)\n",
    "        # Strong prior for points inside polygon\n",
    "        prior_probs[mask] = (1 - prior_strength) / (\n",
    "            k - 1\n",
    "        )  # small prob for other clusters\n",
    "        prior_probs[mask, idx] = prior_strength  # high prob for this cluster\n",
    "        logging.info(f\"Sector {idx}: {mask.sum()} points with strong prior\")\n",
    "\n",
    "    return prior_probs\n",
    "\n",
    "\n",
    "# Load sector polygons for priors\n",
    "sector_files = sorted(glob.glob(config.get(\"data.sector_prior_pattern\")))\n",
    "sector_selectors = [PolygonROISelector.from_file(f) for f in sector_files]\n",
    "k = len(sector_selectors)  # number of clusters = number of sectors\n",
    "logging.info(f\"Found {k} sector polygons for priors\")\n",
    "\n",
    "# Assign priors\n",
    "prior_probs = assign_spatial_priors_dic(\n",
    "    df, sector_selectors, prior_strength=PRIOR_STRENGTH\n",
    ")\n",
    "\n",
    "# Visualize priors\n",
    "nrows = int(np.ceil(np.sqrt(k)))\n",
    "ncols = int(np.ceil(k / nrows))\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(4 * ncols, 4 * nrows))\n",
    "axes = [axes] if k == 1 else axes.flatten()\n",
    "for cluster in range(k):\n",
    "    axes[cluster].imshow(img, alpha=0.3)\n",
    "    scatter = axes[cluster].scatter(\n",
    "        df[\"x\"],\n",
    "        df[\"y\"],\n",
    "        c=prior_probs[:, cluster],\n",
    "        cmap=\"Reds\",\n",
    "        s=1,\n",
    "        alpha=0.7,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "    )\n",
    "    axes[cluster].set_title(f\"Prior for Cluster {cluster}\")\n",
    "    axes[cluster].xaxis.set_ticks([])\n",
    "    axes[cluster].yaxis.set_ticks([])\n",
    "    plt.colorbar(scatter, ax=axes[cluster])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7105f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PYMC MODEL WITH SPATIAL PRIORS ===\n",
    "with pm.Model(\n",
    "    coords={\"cluster\": range(k), \"feature\": range(n_features), \"obs\": range(ndata)}\n",
    ") as model_with_priors:\n",
    "    # Cluster means (multivariate)\n",
    "    μ = pm.Normal(\"μ\", mu=0, sigma=3, dims=(\"cluster\", \"feature\"))\n",
    "\n",
    "    # Cluster standard deviations (diagonal covariance)\n",
    "    σ = pm.HalfNormal(\"σ\", sigma=1.5, dims=(\"cluster\", \"feature\"))\n",
    "\n",
    "    # Cluster assignments with spatial priors\n",
    "    z = pm.Categorical(\"z\", p=prior_probs, dims=\"obs\")\n",
    "\n",
    "    # Likelihood: each point comes from its assigned cluster\n",
    "    pm.Normal(\"x_obs\", mu=μ[z], sigma=σ[z], observed=X_scaled, dims=(\"obs\", \"feature\"))\n",
    "\n",
    "logging.info(\"Model with spatial priors created\")\n",
    "pm.model_to_graphviz(model_with_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0dbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAMPLE FROM MODEL ===\n",
    "with model_with_priors:\n",
    "    logging.info(\"Starting MCMC sampling...\")\n",
    "    idata = pm.sample(\n",
    "        target_accept=0.9,\n",
    "        draws=1000,\n",
    "        tune=1000,\n",
    "        chains=4,\n",
    "        cores=4,\n",
    "        random_seed=RANDOM_SEED,\n",
    "    )\n",
    "\n",
    "logging.info(\"Sampling completed!\")\n",
    "\n",
    "# --- SAVE OUTPUTS ---\n",
    "# save the posterior InferenceData object\n",
    "fpath = output_dir / f\"{base_name}_pooled_posterior.idata.nc\"\n",
    "az.to_netcdf(idata, fpath)\n",
    "\n",
    "# Save the scaler with joblib\n",
    "joblib.dump(scaler, output_dir / f\"{base_name}_scaler.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d1156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Marginalized mixture model (no discrete z) --> faster sampling, but no direct cluster assignments\n",
    "\n",
    "# with pm.Model(\n",
    "#     coords={\"cluster\": range(k), \"feature\": range(n_features), \"obs\": range(ndata)}\n",
    "# ) as model_marginalized:\n",
    "#     # cluster parameters (continuous only)\n",
    "#     μ = pm.Normal(\"μ\", mu=0, sigma=3, dims=(\"cluster\", \"feature\"))\n",
    "#     σ = pm.HalfNormal(\"σ\", sigma=1.5, dims=(\"cluster\", \"feature\"))\n",
    "\n",
    "#     # register data as pm.Data so you can reuse the model and call pm.set_data\n",
    "#     X_shared = pm.Data(\"X_shared\", X_scaled)  # shape (ndata, n_features)\n",
    "#     priors_shared = pm.Data(\"priors_shared\", prior_probs)  # shape (ndata, k)\n",
    "\n",
    "#     # compute per-observation, per-component log-likelihood (diagonal cov assumed)\n",
    "#     # shapes: X_shared (ndata, 1, feature), μ (1, k, feature) -> broadcast to (ndata, k, feature)\n",
    "#     diff = X_shared[:, None, :] - μ[None, :, :]  # (ndata, k, feature)\n",
    "#     inv_sigma = 1.0 / σ[None, :, :]  # (1, k, feature)\n",
    "#     sq = (diff * inv_sigma) ** 2\n",
    "#     # Normal logpdf per feature: -0.5 * ((x-mu)/σ)^2 - log(σ) - 0.5*log(2π)\n",
    "#     log_comp_feat = (\n",
    "#         -0.5 * sq - pm.math.log(σ)[None, :, :] - 0.5 * pm.math.log(2 * np.pi)\n",
    "#     )\n",
    "#     log_comp = log_comp_feat.sum(axis=2)  # (ndata, k)\n",
    "\n",
    "#     # add log spatial priors and marginalize via logsumexp\n",
    "#     log_priors = pm.math.log(priors_shared + 1e-12)  # (ndata, k)\n",
    "#     log_mixture_per_obs = pm.math.logsumexp(log_priors + log_comp, axis=1)  # (ndata,)\n",
    "\n",
    "#     # add total log-likelihood as a potential (no discrete z variable)\n",
    "#     pm.Potential(\"mixture_likelihood\", log_mixture_per_obs.sum())\n",
    "\n",
    "# logging.info(\"Marginalized mixture model created (no discrete z).\")\n",
    "\n",
    "# # Sampling (now NUTS can be used for μ, σ)\n",
    "# with model_marginalized:\n",
    "#     idata_marg = pm.sample(\n",
    "#         draws=1000,\n",
    "#         tune=1000,\n",
    "#         chains=4,\n",
    "#         cores=4,\n",
    "#         target_accept=0.9,\n",
    "#         random_seed=RANDOM_SEED,\n",
    "#     )\n",
    "\n",
    "# idata = idata_marg\n",
    "\n",
    "# Save posterior and scaler\n",
    "# az.to_netcdf(idata_marg, output_dir / f\"{base_name}_pooled_posterior_marginal.idata.nc\")\n",
    "# joblib.dump(scaler, output_dir / f\"{base_name}_scaler.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d562547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check R-hat (should be < 1.01)\n",
    "logging.info(az.rhat(idata))\n",
    "\n",
    "# Check effective sample size (should be > 100)\n",
    "logging.info(az.ess(idata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07d8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot trace plots\n",
    "az.plot_trace(idata, var_names=[\"μ\", \"σ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster assignments\n",
    "z_posterior = idata.posterior[\"z\"]\n",
    "z_samples = z_posterior.values.reshape(-1, z_posterior.shape[-1])\n",
    "\n",
    "cluster_pred = np.zeros(z_posterior.shape[-1], dtype=int)\n",
    "for i in range(z_posterior.shape[-1]):\n",
    "    values, counts = np.unique(z_samples[:, i], return_counts=True)\n",
    "    cluster_pred[i] = values[np.argmax(counts)]\n",
    "\n",
    "# Get model parameters\n",
    "mu_posterior = idata.posterior[\"μ\"].mean(dim=[\"chain\", \"draw\"]).values.flatten()\n",
    "sigma_posterior = idata.posterior[\"σ\"].mean(dim=[\"chain\", \"draw\"]).values.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d66203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 1D velocity clustering\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy.stats import norm as scipy_norm\n",
    "\n",
    "\n",
    "def plot_1d_velocity_clustering(df_features, idata, img, scaler=None):\n",
    "    \"\"\"Specialized plot for 1D velocity-only clustering.\"\"\"\n",
    "\n",
    "    # Get cluster assignments\n",
    "    z_posterior = idata.posterior[\"z\"]\n",
    "    z_samples = z_posterior.values.reshape(-1, z_posterior.shape[-1])\n",
    "\n",
    "    cluster_pred = np.zeros(z_posterior.shape[-1], dtype=int)\n",
    "    for i in range(z_posterior.shape[-1]):\n",
    "        values, counts = np.unique(z_samples[:, i], return_counts=True)\n",
    "        cluster_pred[i] = values[np.argmax(counts)]\n",
    "\n",
    "    # Get model parameters\n",
    "    mu_posterior = idata.posterior[\"μ\"].mean(dim=[\"chain\", \"draw\"]).values.flatten()\n",
    "    sigma_posterior = idata.posterior[\"σ\"].mean(dim=[\"chain\", \"draw\"]).values.flatten()\n",
    "\n",
    "    # Distinct colors\n",
    "    unique_labels = np.unique(cluster_pred)\n",
    "    colors = [\"#E31A1C\", \"#1F78B4\", \"#33A02C\", \"#FF7F00\", \"#6A3D9A\"][\n",
    "        : len(unique_labels)\n",
    "    ]\n",
    "    color_map = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    # Create figure with custom layout\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(8, 12))\n",
    "\n",
    "    # Plot 1: Spatial clusters\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.set_title(\"Velocity-Based Spatial Clustering\", fontsize=14, pad=10)\n",
    "\n",
    "    if img is not None:\n",
    "        ax1.imshow(img, alpha=0.3, cmap=\"gray\")\n",
    "\n",
    "    for label in unique_labels:\n",
    "        mask = cluster_pred == label\n",
    "        if np.any(mask):\n",
    "            ax1.scatter(\n",
    "                df_features.loc[mask, \"x\"],\n",
    "                df_features.loc[mask, \"y\"],\n",
    "                c=color_map[label],\n",
    "                s=8,\n",
    "                alpha=0.8,\n",
    "                label=f\"Velocity Cluster {label}\",\n",
    "                edgecolors=\"none\",\n",
    "            )\n",
    "    ax1.legend(loc=\"upper right\", framealpha=0.9, fontsize=10)\n",
    "    ax1.set_aspect(\"equal\")\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "\n",
    "    # Plot 2: Velocity distribution\n",
    "    ax3 = axes[0, 1]\n",
    "    ax3.set_title(\"Velocity Distribution by Cluster\", fontsize=14, pad=10)\n",
    "\n",
    "    # Plot histograms for each cluster\n",
    "    velocity = df_features[\"V\"].values\n",
    "    for label in unique_labels:\n",
    "        mask = cluster_pred == label\n",
    "        if np.any(mask):\n",
    "            ax3.hist(\n",
    "                velocity[mask],\n",
    "                bins=35,\n",
    "                alpha=0.7,\n",
    "                density=True,\n",
    "                color=color_map[label],\n",
    "                label=f\"Cluster {label}\",\n",
    "                edgecolor=\"white\",\n",
    "                linewidth=0.5,\n",
    "            )\n",
    "\n",
    "    # Overlay model distributions\n",
    "    v_range = np.linspace(velocity.min(), velocity.max(), 200)\n",
    "    for label in unique_labels:\n",
    "        if scaler is not None:\n",
    "            mu_orig = scaler.inverse_transform([[mu_posterior[label]]])[0, 0]\n",
    "            sigma_orig = sigma_posterior[label] * scaler.scale_[0]\n",
    "        else:\n",
    "            mu_orig = mu_posterior[label]\n",
    "            sigma_orig = sigma_posterior[label]\n",
    "\n",
    "        model_dist = scipy_norm.pdf(v_range, mu_orig, sigma_orig)\n",
    "        ax3.plot(\n",
    "            v_range,\n",
    "            model_dist,\n",
    "            \"--\",\n",
    "            color=color_map[label],\n",
    "            linewidth=2.5,\n",
    "            alpha=0.9,\n",
    "            label=f\"Model {label}\",\n",
    "        )\n",
    "    ax3.set_xlabel(\"Velocity Magnitude\", fontsize=12)\n",
    "    ax3.set_ylabel(\"Density\", fontsize=12)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend(fontsize=10, framealpha=0.9)\n",
    "\n",
    "    # Plot 3: Velocity field with quiver plot\n",
    "    ax2 = axes[1, 0]\n",
    "    ax2.set_title(\"Velocity Vector Field\", fontsize=14, pad=10)\n",
    "\n",
    "    if img is not None:\n",
    "        ax2.imshow(img, alpha=0.7, cmap=\"gray\")\n",
    "\n",
    "    # Create quiver plot\n",
    "    magnitudes = df_features[\"V\"].to_numpy()\n",
    "    vmin = 0.0\n",
    "    vmax = np.max(magnitudes)\n",
    "    norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "    q = ax2.quiver(\n",
    "        df_features[\"x\"].to_numpy(),\n",
    "        df_features[\"y\"].to_numpy(),\n",
    "        df_features[\"u\"].to_numpy(),\n",
    "        df_features[\"v\"].to_numpy(),\n",
    "        magnitudes,\n",
    "        scale=None,\n",
    "        scale_units=\"xy\",\n",
    "        angles=\"xy\",\n",
    "        cmap=\"viridis\",\n",
    "        norm=norm,\n",
    "        width=0.003,\n",
    "        headwidth=2.5,\n",
    "        alpha=1.0,\n",
    "    )\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = fig.colorbar(q, ax=ax2, shrink=0.8, aspect=20, pad=0.02)\n",
    "    cbar.set_label(\"Velocity Magnitude\", rotation=270, labelpad=15)\n",
    "    ax2.set_aspect(\"equal\")\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "    ax2.grid(False)\n",
    "\n",
    "    # Plot 4: Statistics\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis(\"off\")\n",
    "    stats_text = \"VELOCITY CLUSTERING STATISTICS\\n\" + \"=\" * 30 + \"\\n\"\n",
    "    for label in unique_labels:\n",
    "        mask = cluster_pred == label\n",
    "        count = mask.sum()\n",
    "\n",
    "        if count == 0:\n",
    "            continue\n",
    "\n",
    "        v_mean = velocity[mask].mean()\n",
    "        v_std = velocity[mask].std()\n",
    "        v_median = np.median(velocity[mask])\n",
    "        nmad = np.median(np.abs(velocity[mask] - v_median)) * 1.4826\n",
    "\n",
    "        # Model parameters (in original scale)\n",
    "        if scaler is not None:\n",
    "            model_mu = scaler.inverse_transform([[mu_posterior[label]]])[0, 0]\n",
    "            model_sigma = sigma_posterior[label] * scaler.scale_[0]\n",
    "        else:\n",
    "            model_mu = mu_posterior[label]\n",
    "            model_sigma = sigma_posterior[label]\n",
    "\n",
    "        stats_text += f\"VELOCITY CLUSTER {label} (pts: {count})\\n\"\n",
    "        stats_text += f\"├─ Velocity: {v_mean:.4f} ± {v_std:.4f}\\n\"\n",
    "        stats_text += f\"├─ Median/NMAD: {v_median:.4f}/{nmad:.4f}\\n\"\n",
    "        stats_text += f\"├─ Model μ/σ: {model_mu:.4f}/{model_sigma:.4f}\\n\\n\"\n",
    "\n",
    "    ax4.text(\n",
    "        0.05,\n",
    "        0.95,\n",
    "        stats_text,\n",
    "        transform=ax4.transAxes,\n",
    "        fontsize=8,\n",
    "        verticalalignment=\"top\",\n",
    "        fontfamily=\"monospace\",\n",
    "        bbox=dict(\n",
    "            boxstyle=\"round,pad=0.4\", facecolor=\"lightblue\", alpha=0.8, edgecolor=\"navy\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return cluster_pred, fig\n",
    "\n",
    "\n",
    "cluster_pred_1d, fig_1d = plot_1d_velocity_clustering(df_features, idata, img, scaler)\n",
    "fig_1d.savefig(\n",
    "    output_dir / f\"{base_name}_velocity_only_results.png\", dpi=300, bbox_inches=\"tight\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppcx-domains (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
