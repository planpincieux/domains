{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b2c551",
   "metadata": {},
   "source": [
    "## CLASSIGY NEW DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e564560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import arviz as az\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from src.clustering import preproc_features\n",
    "from src.config import ConfigManager\n",
    "from src.database import (\n",
    "    get_dic_analysis_by_ids,\n",
    "    get_dic_analysis_ids,\n",
    "    get_dic_data,\n",
    "    get_image,\n",
    "    get_multi_dic_data,\n",
    ")\n",
    "from src.preprocessing import apply_dic_filters, spatial_subsample\n",
    "from src.roi import PolygonROISelector\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "%matplotlib widget\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "\n",
    "RANDOM_SEED = 8927\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "# Load configuration\n",
    "config = ConfigManager()\n",
    "db_engine = create_engine(config.db_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323768ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_date = \"2024-08-23\"\n",
    "camera_name = \"PPCX_Tele\"\n",
    "output_dir = Path(\"output\") / f\"{camera_name}_PyMC\"\n",
    "base_name = f\"{camera_name}_{target_date}_PyMC_GMM\"\n",
    "\n",
    "SUBSAMPLE_FACTOR = 2  # Take every n point\n",
    "SUBSAMPLE_METHOD = \"regular\"  # or 'random', 'stratified'\n",
    "PRIOR_STRENGTH = 0.4  # Between 0 and 1\n",
    "\n",
    "# Load posterior and scaler\n",
    "idata = az.from_netcdf(output_dir / f\"{base_name}_pooled_posterior.idata.nc\")\n",
    "scaler = joblib.load(output_dir / f\"{base_name}_scaler.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e27085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DIC data\n",
    "dic_ids = get_dic_analysis_ids(\n",
    "    db_engine, camera_name=camera_name, reference_date=target_date\n",
    ")\n",
    "if len(dic_ids) == 0:\n",
    "    raise ValueError(\"No DIC analyses found for the given criteria\")\n",
    "elif len(dic_ids) > 1:\n",
    "    logging.warning(\n",
    "        \"Multiple DIC analyses found for the given criteria. Using the first one.\"\n",
    "    )\n",
    "dic_analyses = get_dic_analysis_by_ids(db_engine=db_engine, dic_ids=dic_ids)\n",
    "master_image_id = dic_analyses[\"master_image_id\"].iloc[0]\n",
    "img = get_image(master_image_id, camera_name=camera_name)\n",
    "df = get_dic_data(dic_ids[0])\n",
    "df = apply_dic_filters(\n",
    "    df,\n",
    "    filter_outliers=config.get(\"dic.filter_outliers\"),\n",
    "    tails_percentile=config.get(\"dic.tails_percentile\"),\n",
    ")\n",
    "\n",
    "# Apply ROI filter\n",
    "selector = PolygonROISelector.from_file(config.get(\"data.roi_path\"))\n",
    "df = selector.filter_dataframe(df, x_col=\"x\", y_col=\"y\")\n",
    "logging.info(f\"Data shape after filtering: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_spatial_priors_dic(df, selectors, prior_strength=0.8):\n",
    "    \"\"\"Assign spatial prior probabilities based on polygon sectors.\"\"\"\n",
    "    ndata = len(df)\n",
    "    k = len(selectors)\n",
    "    prior_probs = np.ones((ndata, k)) / k  # default uniform\n",
    "\n",
    "    for idx, selector in enumerate(selectors):\n",
    "        mask = selector.contains_points(df[\"x\"].values, df[\"y\"].values)\n",
    "        # Strong prior for points inside polygon\n",
    "        prior_probs[mask] = (1 - prior_strength) / (\n",
    "            k - 1\n",
    "        )  # small prob for other clusters\n",
    "        prior_probs[mask, idx] = prior_strength  # high prob for this cluster\n",
    "        logging.info(f\"Sector {idx}: {mask.sum()} points with strong prior\")\n",
    "\n",
    "    return prior_probs\n",
    "\n",
    "\n",
    "# Prepare new data\n",
    "variables_names = config.get(\"clustering.variables_names\")\n",
    "df_features = preproc_features(df)\n",
    "X = df_features[variables_names].values\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "\n",
    "# Assign spatial priors\n",
    "sector_files = sorted(glob.glob(config.get(\"data.sector_prior_pattern\")))\n",
    "sector_selectors = [PolygonROISelector.from_file(f) for f in sector_files]\n",
    "k = len(sector_selectors)  # number of clusters = number of sectors\n",
    "logging.info(f\"Found {k} sector polygons for priors\")\n",
    "\n",
    "prior_probs = assign_spatial_priors_dic(\n",
    "    df, sector_selectors, prior_strength=PRIOR_STRENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc116246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior samples\n",
    "mu_samples = idata.posterior[\"μ\"].values  # (chains, draws, k, n_features)\n",
    "sigma_samples = idata.posterior[\"σ\"].values\n",
    "\n",
    "mu_flat = mu_samples.reshape(\n",
    "    -1, mu_samples.shape[-2], mu_samples.shape[-1]\n",
    ")  # (n_samples, k, n_features)\n",
    "sigma_flat = sigma_samples.reshape(-1, sigma_samples.shape[-2], sigma_samples.shape[-1])\n",
    "\n",
    "n_samples = mu_flat.shape[0]\n",
    "n_new = X_scaled.shape[0]\n",
    "k = mu_flat.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387555cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute assignment probabilities\n",
    "log_resp = np.zeros((n_samples, n_new, k))\n",
    "for s in range(n_samples):\n",
    "    for kk in range(k):\n",
    "        lp = norm.logpdf(X_scaled, loc=mu_flat[s, kk], scale=sigma_flat[s, kk])\n",
    "        log_lik = lp.sum(axis=1)\n",
    "        log_prior = np.log(prior_probs[:, kk] + 1e-12)\n",
    "        log_resp[s, :, kk] = log_prior + log_lik\n",
    "\n",
    "resp_samples = np.exp(log_resp - log_resp.max(axis=2, keepdims=True))\n",
    "resp_samples /= resp_samples.sum(axis=2, keepdims=True)  # (n_samples, n_new, k)\n",
    "\n",
    "# Average over posterior samples\n",
    "posterior_probs = resp_samples.mean(axis=0)  # (n_new, k)\n",
    "cluster_pred = posterior_probs.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0cffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 1D velocity clustering\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy.stats import norm as scipy_norm\n",
    "\n",
    "\n",
    "def plot_1d_velocity_clustering(\n",
    "    df_features,\n",
    "    img,\n",
    "    *,\n",
    "    idata,\n",
    "    scaler=None,\n",
    "    cluster_pred=None,\n",
    "):\n",
    "    \"\"\"Specialized plot for 1D velocity-only clustering.\"\"\"\n",
    "\n",
    "    # Use provided cluster assignments when available\n",
    "    if cluster_pred is None:\n",
    "        # Try to extract discrete z samples from idata (if present)\n",
    "        if \"z\" in idata.posterior:\n",
    "            z_posterior = idata.posterior[\"z\"]\n",
    "            z_samples = z_posterior.values.reshape(-1, z_posterior.shape[-1])\n",
    "            cluster_pred = np.zeros(z_posterior.shape[-1], dtype=int)\n",
    "            for i in range(z_posterior.shape[-1]):\n",
    "                values, counts = np.unique(z_samples[:, i], return_counts=True)\n",
    "                cluster_pred[i] = values[np.argmax(counts)]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"cluster_pred not provided and 'z' not found in idata.posterior\"\n",
    "            )\n",
    "\n",
    "    # Get model parameters\n",
    "    mu_posterior = idata.posterior[\"μ\"].mean(dim=[\"chain\", \"draw\"]).values.flatten()\n",
    "    sigma_posterior = idata.posterior[\"σ\"].mean(dim=[\"chain\", \"draw\"]).values.flatten()\n",
    "\n",
    "    # Distinct colors\n",
    "    unique_labels = np.unique(cluster_pred)\n",
    "    colors = [\"#E31A1C\", \"#1F78B4\", \"#33A02C\", \"#FF7F00\", \"#6A3D9A\"][\n",
    "        : len(unique_labels)\n",
    "    ]\n",
    "    color_map = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    # Create figure with custom layout\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(8, 12))\n",
    "\n",
    "    # Plot 1: Spatial clusters\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.set_title(\"Velocity-Based Spatial Clustering\", fontsize=14, pad=10)\n",
    "\n",
    "    if img is not None:\n",
    "        ax1.imshow(img, alpha=0.3, cmap=\"gray\")\n",
    "\n",
    "    for label in unique_labels:\n",
    "        mask = cluster_pred == label\n",
    "        if np.any(mask):\n",
    "            ax1.scatter(\n",
    "                df_features.loc[mask, \"x\"],\n",
    "                df_features.loc[mask, \"y\"],\n",
    "                c=color_map[label],\n",
    "                s=8,\n",
    "                alpha=0.8,\n",
    "                label=f\"Velocity Cluster {label}\",\n",
    "                edgecolors=\"none\",\n",
    "            )\n",
    "    ax1.legend(loc=\"upper right\", framealpha=0.9, fontsize=10)\n",
    "    ax1.set_aspect(\"equal\")\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "\n",
    "    # Plot 2: Velocity distribution\n",
    "    ax3 = axes[0, 1]\n",
    "    ax3.set_title(\"Velocity Distribution by Cluster\", fontsize=14, pad=10)\n",
    "\n",
    "    # Plot histograms for each cluster\n",
    "    velocity = df_features[\"V\"].values\n",
    "    for label in unique_labels:\n",
    "        mask = cluster_pred == label\n",
    "        if np.any(mask):\n",
    "            ax3.hist(\n",
    "                velocity[mask],\n",
    "                bins=35,\n",
    "                alpha=0.7,\n",
    "                density=True,\n",
    "                color=color_map[label],\n",
    "                label=f\"Cluster {label}\",\n",
    "                edgecolor=\"white\",\n",
    "                linewidth=0.5,\n",
    "            )\n",
    "\n",
    "    # Overlay model distributions\n",
    "    v_range = np.linspace(velocity.min(), velocity.max(), 200)\n",
    "    for label in unique_labels:\n",
    "        if scaler is not None:\n",
    "            mu_orig = scaler.inverse_transform([[mu_posterior[label]]])[0, 0]\n",
    "            sigma_orig = sigma_posterior[label] * scaler.scale_[0]\n",
    "        else:\n",
    "            mu_orig = mu_posterior[label]\n",
    "            sigma_orig = sigma_posterior[label]\n",
    "\n",
    "        model_dist = scipy_norm.pdf(v_range, mu_orig, sigma_orig)\n",
    "        ax3.plot(\n",
    "            v_range,\n",
    "            model_dist,\n",
    "            \"--\",\n",
    "            color=color_map[label],\n",
    "            linewidth=2.5,\n",
    "            alpha=0.9,\n",
    "            label=f\"Model {label}\",\n",
    "        )\n",
    "    ax3.set_xlabel(\"Velocity Magnitude\", fontsize=12)\n",
    "    ax3.set_ylabel(\"Density\", fontsize=12)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend(fontsize=10, framealpha=0.9)\n",
    "\n",
    "    # Plot 3: Velocity field with quiver plot\n",
    "    ax2 = axes[1, 0]\n",
    "    ax2.set_title(\"Velocity Vector Field\", fontsize=14, pad=10)\n",
    "\n",
    "    if img is not None:\n",
    "        ax2.imshow(img, alpha=0.7, cmap=\"gray\")\n",
    "\n",
    "    # Create quiver plot\n",
    "    magnitudes = df_features[\"V\"].to_numpy()\n",
    "    vmin = 0.0\n",
    "    vmax = np.max(magnitudes)\n",
    "    norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "    q = ax2.quiver(\n",
    "        df_features[\"x\"].to_numpy(),\n",
    "        df_features[\"y\"].to_numpy(),\n",
    "        df_features[\"u\"].to_numpy(),\n",
    "        df_features[\"v\"].to_numpy(),\n",
    "        magnitudes,\n",
    "        scale=None,\n",
    "        scale_units=\"xy\",\n",
    "        angles=\"xy\",\n",
    "        cmap=\"viridis\",\n",
    "        norm=norm,\n",
    "        width=0.003,\n",
    "        headwidth=2.5,\n",
    "        alpha=1.0,\n",
    "    )\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = fig.colorbar(q, ax=ax2, shrink=0.8, aspect=20, pad=0.02)\n",
    "    cbar.set_label(\"Velocity Magnitude\", rotation=270, labelpad=15)\n",
    "    ax2.set_aspect(\"equal\")\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "    ax2.grid(False)\n",
    "\n",
    "    # Plot 4: Statistics\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis(\"off\")\n",
    "    stats_text = \"VELOCITY CLUSTERING STATISTICS\\n\" + \"=\" * 30 + \"\\n\"\n",
    "    for label in unique_labels:\n",
    "        mask = cluster_pred == label\n",
    "        count = mask.sum()\n",
    "\n",
    "        if count == 0:\n",
    "            continue\n",
    "\n",
    "        v_mean = velocity[mask].mean()\n",
    "        v_std = velocity[mask].std()\n",
    "        v_median = np.median(velocity[mask])\n",
    "        nmad = np.median(np.abs(velocity[mask] - v_median)) * 1.4826\n",
    "\n",
    "        # Model parameters (in original scale)\n",
    "        if scaler is not None:\n",
    "            model_mu = scaler.inverse_transform([[mu_posterior[label]]])[0, 0]\n",
    "            model_sigma = sigma_posterior[label] * scaler.scale_[0]\n",
    "        else:\n",
    "            model_mu = mu_posterior[label]\n",
    "            model_sigma = sigma_posterior[label]\n",
    "\n",
    "        stats_text += f\"VELOCITY CLUSTER {label} (pts: {count})\\n\"\n",
    "        stats_text += f\"├─ Velocity: {v_mean:.4f} ± {v_std:.4f}\\n\"\n",
    "        stats_text += f\"├─ Median/NMAD: {v_median:.4f}/{nmad:.4f}\\n\"\n",
    "        stats_text += f\"├─ Model μ/σ: {model_mu:.4f}/{model_sigma:.4f}\\n\\n\"\n",
    "\n",
    "    ax4.text(\n",
    "        0.05,\n",
    "        0.95,\n",
    "        stats_text,\n",
    "        transform=ax4.transAxes,\n",
    "        fontsize=8,\n",
    "        verticalalignment=\"top\",\n",
    "        fontfamily=\"monospace\",\n",
    "        bbox=dict(\n",
    "            boxstyle=\"round,pad=0.4\", facecolor=\"lightblue\", alpha=0.8, edgecolor=\"navy\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return cluster_pred, fig\n",
    "\n",
    "\n",
    "cluster_pred_1d, fig_1d = plot_1d_velocity_clustering(\n",
    "    df_features, img, idata=idata, scaler=scaler, cluster_pred=cluster_pred\n",
    ")\n",
    "# fig_1d.savefig(\n",
    "#     output_dir / f\"{base_name}_velocity_only_results.png\", dpi=300, bbox_inches=\"tight\"\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppcx-domains (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
