{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e564560",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import arviz as az\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    adjusted_mutual_info_score,\n",
    "    adjusted_rand_score,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from ppcluster import logger, mcmc\n",
    "from ppcluster.cvat import (\n",
    "    filter_dataframe_by_polygons,\n",
    "    read_polygons_from_cvat,\n",
    ")\n",
    "from ppcluster.griddata import create_2d_grid, map_grid_to_points\n",
    "from ppcluster.mcmc.postproc import (\n",
    "    aggregate_multiscale_clustering,\n",
    "    remove_small_grid_components,\n",
    ")\n",
    "from ppcluster.preprocessing import (\n",
    "    apply_2d_gaussian_filter,\n",
    "    apply_dic_filters,\n",
    "    preprocess_velocity_features,\n",
    "    spatial_subsample,\n",
    ")\n",
    "from ppcluster.utils.config import ConfigManager\n",
    "from ppcluster.utils.database import (\n",
    "    get_dic_analysis_by_ids,\n",
    "    get_dic_analysis_ids,\n",
    "    get_image,\n",
    "    get_multi_dic_data,\n",
    ")\n",
    "\n",
    "RANDOM_SEED = 8927\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# Load configuration\n",
    "config = ConfigManager()\n",
    "db_engine = create_engine(config.db_url)\n",
    "\n",
    "\n",
    "SAVE_OUTPUTS = True  # Set to True to save inference results\n",
    "LOAD_EXISTING = False  # Set to False to run sampling again\n",
    "\n",
    "# MCMC parameters\n",
    "DRAWS = 2000  # Number of MCMC draws\n",
    "TUNE = 1000  # Number of tuning steps\n",
    "CHAINS = 4  # Number of MCMC chains\n",
    "CORES = 4  # Number of CPU cores to use\n",
    "TARGET_ACCEPT = 0.9  # Target acceptance rate for NUTS sampler\n",
    "\n",
    "# Data selection parameters\n",
    "camera_name = \"PPCX_Tele\"\n",
    "reference_date = None  # \"2024-08-02\"\n",
    "reference_start_date = \"2024-08-02\"  #\n",
    "reference_end_date = \"2024-08-02\"  #\n",
    "dt_min = 72  # Minimum time difference between images in hours\n",
    "dt_max = 96  # Maximum time difference between images in hours\n",
    "# dt_min = 24  # 72  # Minimum time difference between images in hours\n",
    "# dt_max = 200  # 96  # Maximum time difference between images in hours\n",
    "\n",
    "SUBSAMPLE_FACTOR = 1  # 1=Take every n point\n",
    "SUBSAMPLE_METHOD = \"random\"  # or 'random', 'stratified'\n",
    "\n",
    "# Parse various parameters from config file (or set manually here)\n",
    "variables_names = [\"V\"]\n",
    "filter_kwargs = dict(\n",
    "    filter_outliers=False,\n",
    "    tails_percentile=0.005,\n",
    "    min_velocity=0.0,\n",
    "    apply_2d_median=False,\n",
    "    median_window_size=5,\n",
    "    median_threshold_factor=3,\n",
    "    apply_2d_gaussian=False,\n",
    "    gaussian_sigma=1.0,\n",
    ")\n",
    "\n",
    "# == PRIORS and ROI ==\n",
    "# Define a specific prior probability for each sector (overrides PRIOR_STRENGTH)\n",
    "# This is a dictionary where keys are sector names and values are lists of prior probabilities (Sector names must match those in the XML file)\n",
    "# Sector name: [P(Cluster A), P(Cluster B), P(Cluster C)...]\n",
    "# PRIOR_PROBABILITY = {\n",
    "#     \"A\": [1.0, 0.0, 0.0],\n",
    "#     \"B\": [0.1, 0.7, 0.2],\n",
    "#     \"C\": [0.0, 0.2, 0.8],\n",
    "# }\n",
    "SECTOR_PRIOR_FILE = Path(\"data/priors_4_sectors.xml\")\n",
    "PRIOR_PROBABILITY = {\n",
    "    \"A\": [1.0, 0.0, 0.0, 0.0],\n",
    "    \"B\": [0.1, 0.7, 0.2, 0.0],\n",
    "    \"C\": [0.0, 0.2, 0.8, 0.0],\n",
    "    \"D\": [0.0, 0.0, 0.3, 0.7],\n",
    "}\n",
    "\n",
    "roi_path = Path(\"data/roi.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4b243d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 18:37:43 | [INFO    ] Found 1 DIC analyses matching criteria\n",
      "2025-10-01 18:37:43 | [INFO    ] Fetched DIC analysis:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIC ID: 1915, date: 2024-08-02, dt (hrs): 72, Master: 2024-07-30 05:00:18+00:00, Slave: 2024-08-02 05:00:18+00:00\n",
      "Summary of selected DIC analyses:\n",
      "       dic_id  master_image_id  slave_image_id  dt_hours\n",
      "count     1.0              1.0             1.0       1.0\n",
      "mean   1915.0          34633.0         34670.0      72.0\n",
      "std       NaN              NaN             NaN       NaN\n",
      "min    1915.0          34633.0         34670.0      72.0\n",
      "25%    1915.0          34633.0         34670.0      72.0\n",
      "50%    1915.0          34633.0         34670.0      72.0\n",
      "75%    1915.0          34633.0         34670.0      72.0\n",
      "max    1915.0          34633.0         34670.0      72.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 18:37:43 | [INFO    ] Fetched DIC data for id 1915 with 3927 points\n",
      "2025-10-01 18:37:43 | [INFO    ] Found stack of 1 DIC dataframes. Run filtering...\n",
      "2025-10-01 18:37:43 | [INFO    ] Starting DIC filtering pipeline with 2659 points\n",
      "2025-10-01 18:37:43 | [INFO    ] Min velocity filtering: 2659 -> 2659 points (removed 0 points below 0.0)\n",
      "2025-10-01 18:37:43 | [INFO    ] DIC filtering pipeline completed: 2659 -> 2659 points (removed 0 total)\n",
      "2025-10-01 18:37:43 | [INFO    ] Data shape after filtering and stacking: (2659, 5)\n"
     ]
    }
   ],
   "source": [
    "# Read roi and spatial priors\n",
    "roi = read_polygons_from_cvat(roi_path, image_name=None)\n",
    "sectors = read_polygons_from_cvat(SECTOR_PRIOR_FILE, image_name=None)\n",
    "\n",
    "# Check that at least the reference date or an interval of dates is provided\n",
    "if not (reference_date or (reference_start_date and reference_end_date)):\n",
    "    raise ValueError(\n",
    "        \"Either reference_date or both reference_start_date and reference_end_date must be provided.\"\n",
    "    )\n",
    "\n",
    "# Fetch DIC ids\n",
    "dic_ids = get_dic_analysis_ids(\n",
    "    db_engine,\n",
    "    camera_name=camera_name,\n",
    "    reference_date=reference_date,\n",
    "    reference_date_start=reference_start_date,\n",
    "    reference_date_end=reference_end_date,\n",
    "    time_difference_min=dt_min,\n",
    "    time_difference_max=dt_max,\n",
    ")\n",
    "if len(dic_ids) < 1:\n",
    "    raise ValueError(\"No DIC analyses found for the given criteria\")\n",
    "\n",
    "# Get DIC analysis metadata\n",
    "dic_analyses = get_dic_analysis_by_ids(db_engine=db_engine, dic_ids=dic_ids)\n",
    "logger.info(\"Fetched DIC analysis:\")\n",
    "for _, row in dic_analyses.iterrows():\n",
    "    print(\n",
    "        f\"DIC ID: {row['dic_id']}, date: {row['reference_date']}, dt (hrs): {row['dt_hours']}, Master: {row['master_timestamp']}, Slave: {row['slave_timestamp']}\"\n",
    "    )\n",
    "print(\"Summary of selected DIC analyses:\")\n",
    "print(dic_analyses.describe())\n",
    "\n",
    "\n",
    "# Output paths\n",
    "date_start = dic_analyses.iloc[0][\"master_timestamp\"].strftime(\"%Y-%m-%d\")\n",
    "date_end = dic_analyses.iloc[0][\"slave_timestamp\"].strftime(\"%Y-%m-%d\")\n",
    "output_dir = Path(\"output\") / f\"{camera_name}_{date_end}_mcmc_multiscale\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "base_name = f\"{date_start}_{date_end}\"\n",
    "\n",
    "# Get master image\n",
    "master_image_id = dic_analyses[\"master_image_id\"].iloc[0]\n",
    "img = get_image(image_id=master_image_id, config=config)\n",
    "\n",
    "# Fetch DIC data\n",
    "out = get_multi_dic_data(\n",
    "    dic_ids,\n",
    "    stack_results=False,\n",
    "    config=config,\n",
    ")\n",
    "logger.info(f\"Found stack of {len(out)} DIC dataframes. Run filtering...\")\n",
    "\n",
    "# Apply filter for each df in the dictionary and then stack them\n",
    "processed = []\n",
    "for src_id, df_src in out.items():\n",
    "    try:\n",
    "        # Filter only points inside the spatial priors sectors\n",
    "        df_src = filter_dataframe_by_polygons(df_src, polygons=roi)\n",
    "\n",
    "        # Apply other DIC filters if any\n",
    "        df_src = apply_dic_filters(df_src, **filter_kwargs)\n",
    "\n",
    "        # Append processed dataframe to the list\n",
    "        processed.append(df_src)\n",
    "    except Exception as exc:\n",
    "        logger.warning(\"Filtering failed for %s: %s\", src_id, exc)\n",
    "if not processed:\n",
    "    raise RuntimeError(\"No dataframes left after filtering.\")\n",
    "# Stack all processed dataframes\n",
    "df = pd.concat(processed, ignore_index=True)\n",
    "logger.info(\"Data shape after filtering and stacking: %s\", df.shape)\n",
    "\n",
    "# Apply subsampling\n",
    "if SUBSAMPLE_FACTOR > 1:\n",
    "    df_subsampled = spatial_subsample(\n",
    "        df, n_subsample=SUBSAMPLE_FACTOR, method=SUBSAMPLE_METHOD\n",
    "    )\n",
    "    df = df_subsampled\n",
    "    logger.info(f\"Data shape after subsampling: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c71629",
   "metadata": {},
   "source": [
    "## RUN MCMC multiple times with different smoothing scales and median the clustering results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ca00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign spatial priors\n",
    "prior_probs = mcmc.assign_spatial_priors(\n",
    "    x=df[\"x\"].to_numpy(),\n",
    "    y=df[\"y\"].to_numpy(),\n",
    "    polygons=sectors,\n",
    "    prior_probs=PRIOR_PROBABILITY,\n",
    "    method=\"exponential\",\n",
    "    method_kws={\"decay_rate\": 0.001},\n",
    ")\n",
    "\n",
    "fig, axes = mcmc.plot_spatial_priors(df, prior_probs, img=img)\n",
    "fig.savefig(\n",
    "    output_dir / f\"{base_name}_spatial_priors.jpg\",\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.close(fig)\n",
    "\n",
    "# Plot velocity field\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "mcmc.plot_velocity_magnitude(\n",
    "    df[\"x\"].to_numpy(),\n",
    "    df[\"y\"].to_numpy(),\n",
    "    df[\"V\"].to_numpy(),\n",
    "    img=img,\n",
    "    ax=ax,\n",
    ")\n",
    "fig.savefig(\n",
    "    output_dir / f\"{base_name}_velocity_field.jpg\",\n",
    "    dpi=150,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function to preprocess velocity features\n",
    "\n",
    "\n",
    "def run_mcmc_clustering(\n",
    "    df_input,\n",
    "    prior_probs,\n",
    "    sectors,\n",
    "    output_dir,\n",
    "    base_name,\n",
    "    img=None,\n",
    "    variables_names=None,\n",
    "    transform_velocity=\"none\",\n",
    "    transform_params=None,\n",
    "    mu_params=None,\n",
    "    sigma_params=None,\n",
    "    feature_weights=None,\n",
    "    sample_args=None,\n",
    "    mrf_regularization: bool = False,\n",
    "    mrf_kwargs: dict | None = None,\n",
    "    second_pass: str = \"full\",  # \"skip\" | \"short\" | \"full\"\n",
    "    second_pass_sample_args: dict | None = None,\n",
    "    random_seed=8927,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run MCMC-based clustering on velocity data with flexible velocity transformations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_input : pandas.DataFrame\n",
    "        Input dataframe with 'x', 'y', 'V' columns\n",
    "    transform_velocity : str, default=\"none\"\n",
    "        Type of velocity transformation: \"power\", \"exponential\", \"threshold\", \"sigmoid\", or \"none\"\n",
    "    transform_params : dict, optional\n",
    "        Parameters for velocity transformation (see preprocess_velocity_features for details)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- helper: build initvals from idata posterior means (warm-start) ---\n",
    "    def _initvals_from_idata(idata_in, n_chains):\n",
    "        mu_mean = idata_in.posterior[\"mu\"].mean(dim=[\"chain\", \"draw\"]).values\n",
    "        sigma_mean = idata_in.posterior[\"sigma\"].mean(dim=[\"chain\", \"draw\"]).values\n",
    "        # Ensure shapes match the model dims; return a list of per-chain dicts\n",
    "        init = {\"mu\": mu_mean, \"sigma\": sigma_mean}\n",
    "        return [init for _ in range(n_chains)]\n",
    "\n",
    "    logger.info(f\"Running MCMC clustering for {base_name}...\")\n",
    "\n",
    "    # Default parameters if not provided\n",
    "    if mu_params is None:\n",
    "        mu_params = {\"mu\": 0, \"sigma\": 1}\n",
    "    if sigma_params is None:\n",
    "        sigma_params = {\"sigma\": 1}\n",
    "    if sample_args is None:\n",
    "        sample_args = dict(\n",
    "            target_accept=0.95,\n",
    "            draws=2000,\n",
    "            tune=1000,\n",
    "            chains=4,\n",
    "            cores=4,\n",
    "            random_seed=random_seed,\n",
    "        )\n",
    "    if variables_names is None:\n",
    "        variables_names = [\"V\"]\n",
    "\n",
    "    if \"V\" not in df_input.columns:\n",
    "        raise ValueError(\"Input dataframe must contain 'V' column for velocities.\")\n",
    "\n",
    "    # Preprocess velocity features to enhance high velocities\n",
    "    velocities, transform_info = preprocess_velocity_features(\n",
    "        velocities=df_input[\"V\"].to_numpy(),\n",
    "        velocity_transform=transform_velocity,\n",
    "        velocity_params=transform_params,\n",
    "    )\n",
    "\n",
    "    # Extract data array for clustering\n",
    "    if len(variables_names) > 1:\n",
    "        # Concatenate other features to velocities\n",
    "        additional_vars = variables_names.copy()\n",
    "        if \"V\" in additional_vars:\n",
    "            additional_vars.remove(\"V\")\n",
    "        additional_data = df_input[additional_vars].to_numpy()\n",
    "        data_array = np.column_stack((velocities, additional_data))\n",
    "    else:\n",
    "        # Use only velocities\n",
    "        data_array = velocities.reshape(-1, 1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data_array)\n",
    "    joblib.dump(scaler, output_dir / f\"{base_name}_scaler.joblib\")\n",
    "\n",
    "    # Scale data for model input\n",
    "    data_array_scaled = scaler.transform(data_array)\n",
    "\n",
    "    # Build model\n",
    "    logger.info(f\"Running MCMC clustering for {base_name}...\")\n",
    "    model = mcmc.build_marginalized_mixture_model(\n",
    "        data_array_scaled,\n",
    "        prior_probs,\n",
    "        sectors,\n",
    "        mu_params=mu_params,\n",
    "        sigma_params=sigma_params,\n",
    "        feature_weights=feature_weights,\n",
    "    )\n",
    "\n",
    "    # Sample model (1st pass)\n",
    "    idata, convergence_flag = mcmc.sample_model(\n",
    "        model, output_dir, base_name, **sample_args\n",
    "    )\n",
    "    if not convergence_flag:\n",
    "        idata_summary = az.summary(idata, var_names=[\"mu\", \"sigma\"])\n",
    "        logger.info(f\"MCMC did not converge. Summary:\\n{idata_summary}\")\n",
    "\n",
    "    # --- MRF regularization of priors and optional re-sample ---\n",
    "    prior_used = prior_probs\n",
    "    if mrf_regularization:\n",
    "        x_pos = df_input[\"x\"].to_numpy()\n",
    "        y_pos = df_input[\"y\"].to_numpy()\n",
    "        mkw = dict(n_neighbors=8, length_scale=50, beta=2.0, n_iter=5)\n",
    "        if mrf_kwargs:\n",
    "            mkw.update(mrf_kwargs)\n",
    "        prior_mrf, q_mrf = mcmc.run_mrf_regularization(\n",
    "            data_array_scaled, idata, prior_probs, x_pos, y_pos, **mkw\n",
    "        )\n",
    "        prior_used = prior_mrf\n",
    "\n",
    "        # visualize refined priors\n",
    "        try:\n",
    "            fig, _ = mcmc.plot_spatial_priors(df_input, prior_mrf, img=img)\n",
    "            fig.savefig(\n",
    "                output_dir / f\"{base_name}_mrf_priors.png\", dpi=150, bbox_inches=\"tight\"\n",
    "            )\n",
    "            plt.close(fig)\n",
    "        except Exception as exc:\n",
    "            logger.warning(f\"Could not plot MRF priors: {exc}\")\n",
    "\n",
    "    # Decide second pass strategy\n",
    "    if mrf_regularization and second_pass.lower() == \"skip\":\n",
    "        # Fastest: don't re-sample. Use q_mrf as final posterior_probs and argmax as labels.\n",
    "        posterior_probs = q_mrf\n",
    "        cluster_pred = np.argmax(posterior_probs, axis=1)\n",
    "        uncertainty = 1.0 - posterior_probs.max(axis=1)\n",
    "        # keep idata from 1st pass for plots/params\n",
    "    else:\n",
    "        # Re-sample with refined priors (short or full)\n",
    "        if mrf_regularization:\n",
    "            with model:\n",
    "                pm.set_data({\"prior_w\": prior_used})\n",
    "\n",
    "        # Allow short second pass and warm start\n",
    "        sp2_args = dict(**sample_args)\n",
    "        if second_pass.lower() == \"short\":\n",
    "            # much fewer draws/tune; fewer chains can also help\n",
    "            sp2_args.update(dict(draws=600, tune=400, chains=2, cores=2))\n",
    "            if second_pass_sample_args:\n",
    "                sp2_args.update(second_pass_sample_args)\n",
    "        elif second_pass_sample_args:\n",
    "            sp2_args.update(second_pass_sample_args)\n",
    "\n",
    "        # Warm-start from previous posterior means\n",
    "        initvals = _initvals_from_idata(idata, sp2_args.get(\"chains\", 2))\n",
    "\n",
    "        with model:\n",
    "            # pass initvals through sample_model if it supports, else call pm.sample directly\n",
    "            try:\n",
    "                idata, convergence_flag = mcmc.sample_model(\n",
    "                    model,\n",
    "                    output_dir,\n",
    "                    base_name + (\"_mrf\" if mrf_regularization else \"\"),\n",
    "                    initvals=initvals,\n",
    "                    **sp2_args,\n",
    "                )\n",
    "            except TypeError:\n",
    "                # fallback if your wrapper doesn't accept initvals\n",
    "                idata = pm.sample(**sp2_args)\n",
    "                convergence_flag = True\n",
    "\n",
    "        # Compute posterior-based assignments\n",
    "        posterior_probs, cluster_pred, uncertainty = mcmc.compute_posterior_assignments(\n",
    "            idata, n_posterior_samples=200\n",
    "        )\n",
    "\n",
    "    # Generate plots\n",
    "    fig = mcmc.plot_velocity_clustering(\n",
    "        df_features=df_input,\n",
    "        img=img,\n",
    "        idata=idata,\n",
    "        cluster_pred=cluster_pred,\n",
    "        posterior_probs=posterior_probs,\n",
    "        scaler=scaler,\n",
    "    )\n",
    "    fig.savefig(\n",
    "        output_dir / f\"{base_name}_results.png\",\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Trace plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 6))\n",
    "    az.plot_trace(\n",
    "        idata, var_names=[\"mu\", \"sigma\"], axes=axes, compact=True, legend=True\n",
    "    )\n",
    "    fig.savefig(output_dir / f\"{base_name}_trace_plots.png\", dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Forest plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    az.plot_forest(idata, var_names=[\"mu\", \"sigma\"], combined=True, ess=True, ax=axes)\n",
    "    fig.savefig(output_dir / f\"{base_name}_forest_plot.png\", dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Collect and save metadata\n",
    "    metadata = mcmc.collect_run_metadata(\n",
    "        idata=idata,\n",
    "        convergence_flag=convergence_flag,\n",
    "        data_array_scaled=data_array_scaled,\n",
    "        variables_names=variables_names,\n",
    "        sectors=sectors,\n",
    "        prior_probs=prior_probs,\n",
    "        sample_args=sample_args,\n",
    "        frame=locals(),\n",
    "    )\n",
    "    mcmc.save_run_metadata(output_dir, base_name, metadata)\n",
    "\n",
    "    # Return results dictionary\n",
    "    result = {\n",
    "        \"metadata\": metadata,\n",
    "        \"idata\": idata,\n",
    "        \"scaler\": scaler,\n",
    "        \"convergence_flag\": convergence_flag,\n",
    "        \"posterior_probs\": posterior_probs,\n",
    "        \"cluster_pred\": cluster_pred,\n",
    "        \"uncertainty\": uncertainty,\n",
    "    }\n",
    "\n",
    "    plt.close(\"all\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# Sample the second derivative at each point's y-coordinate and add as new feature\n",
    "# if \"d2v_dy2\" in variables_names:\n",
    "#     df, d2v_sampled = add_second_derivative_feature(\n",
    "#         df=df,\n",
    "#         y_values=df[\"y\"].to_numpy(),\n",
    "#         bin_centers=bin_centers,\n",
    "#         vel_second_derivative=vel_second_derivative,\n",
    "#         valid_bins=valid_bins,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670eada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sigma values for Gaussian smoothing\n",
    "sigma_values = [2]\n",
    "variables_names = [\"V\"]\n",
    "\n",
    "\n",
    "# Loop through smoothing scales\n",
    "results = []\n",
    "for sigma in sigma_values:\n",
    "    logger.info(f\"Processing with Gaussian smoothing sigma={sigma}...\")\n",
    "\n",
    "    # Create scale-specific base name\n",
    "    scale_base_name = f\"{date_start}_{date_end}_sigma{sigma}\"\n",
    "\n",
    "    # Apply Gaussian smoothing if needed (skipped for sigma=0)\n",
    "    df_run = apply_2d_gaussian_filter(df, sigma=sigma)\n",
    "\n",
    "    # Adjust model parameters based on scale\n",
    "    mu_params = {\"mu\": 0, \"sigma\": 1 if sigma <= 2 else 0.5}\n",
    "    sigma_params = {\"sigma\": 1 if sigma <= 2 else 0.5}\n",
    "\n",
    "    # Run MCMC clustering with the smoothed data\n",
    "    result = run_mcmc_clustering(\n",
    "        df_input=df_run,\n",
    "        prior_probs=prior_probs,\n",
    "        sectors=sectors,\n",
    "        variables_names=variables_names,\n",
    "        output_dir=output_dir,\n",
    "        base_name=scale_base_name,\n",
    "        img=img,\n",
    "        # transform_velocity=\"sigmoid\",\n",
    "        # transform_params={\"midpoint_percentile\": 70, \"steepness\": 2.0},\n",
    "        mu_params=mu_params,\n",
    "        sigma_params=sigma_params,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        mrf_regularization=True,\n",
    "        mrf_kwargs=dict(n_neighbors=8, length_scale=50, beta=2.0, n_iter=5),\n",
    "        # Speed choices:\n",
    "        # 1) \"short\": short sampling + warm-start (recommended default):\n",
    "        # 2) \"skip\": fastest, rely only on MRF priors, no re-sampling\n",
    "        second_pass=\"short\",\n",
    "        second_pass_sample_args=dict(\n",
    "            draws=500, tune=300, chains=4, cores=4, target_accept=0.9\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add scale information to result\n",
    "    result[\"sigma\"] = sigma\n",
    "\n",
    "    # Append to results list\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27612f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to a single joblib file\n",
    "# joblib.dump(\n",
    "#     results,\n",
    "#     output_dir\n",
    "#     / f\"{date_start}_{date_end}_all_results_multiscale.joblib\",\n",
    "# )\n",
    "\n",
    "# Read the results again\n",
    "# results = joblib.load(\n",
    "#     output_dir\n",
    "#     / f\"{date_start}_{date_end}_all_results_multiscale.joblib\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507da71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===  If a multi-scale approach was used, aggregate the results.\n",
    "if len(sigma_values) > 1:\n",
    "    aggregated_results = aggregate_multiscale_clustering(\n",
    "        results,\n",
    "        similarity_threshold=0.7,\n",
    "        overall_threshold=0.8,\n",
    "        fig_path=output_dir\n",
    "        / f\"{reference_start_date}_{reference_end_date}_similarity_heatmap.jpg\",\n",
    "    )\n",
    "\n",
    "    # Unpack aggregated results\n",
    "    cluster_pred = aggregated_results[\"combined_cluster_pred\"]\n",
    "    posterior_probs = aggregated_results[\"avg_posterior_probs\"]\n",
    "    entropy = aggregated_results[\"avg_entropy\"]\n",
    "    similarity_matrix = aggregated_results[\"similarity_matrix\"]\n",
    "    stability_score = aggregated_results[\"stability_score\"]\n",
    "    valid_scales = aggregated_results[\"valid_scales\"]\n",
    "\n",
    "else:\n",
    "    # Otherwise extract the single result\n",
    "    cluster_pred = results[0][\"cluster_pred\"]\n",
    "    posterior_probs = results[0][\"posterior_probs\"]\n",
    "    entropy = -np.sum(posterior_probs * np.log(posterior_probs + 1e-10), axis=1)\n",
    "    similarity_matrix = None\n",
    "    stability_score = None\n",
    "    valid_scales = None\n",
    "\n",
    "\n",
    "# ===  Save final clustering results\n",
    "cluster_aggregation_outs = {\n",
    "    \"cluster_pred\": cluster_pred,\n",
    "    \"posterior_probs\": posterior_probs,\n",
    "    \"entropy\": entropy,\n",
    "    \"similarity_matrix\": similarity_matrix,\n",
    "    \"stability_score\": stability_score,\n",
    "    \"valid_scales\": valid_scales,\n",
    "}\n",
    "joblib.dump(\n",
    "    cluster_aggregation_outs,\n",
    "    output_dir\n",
    "    / f\"{reference_start_date}_{reference_end_date}_kinematic_clustering_results.joblib\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc2d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data again to skip mcmc sampling if already done\n",
    "# cluster_aggregation_outs = joblib.load(\n",
    "#     output_dir\n",
    "#     / f\"{reference_start_date}_{reference_end_date}_kinematic_clustering_results.joblib\",\n",
    "# )\n",
    "# cluster_pred = cluster_aggregation_outs[\"cluster_pred\"]\n",
    "# posterior_probs = cluster_aggregation_outs[\"posterior_probs\"]\n",
    "# entropy = cluster_aggregation_outs[\"entropy\"]\n",
    "# similarity_matrix = cluster_aggregation_outs[\"similarity_matrix\"]\n",
    "# stability_score = cluster_aggregation_outs[\"stability_score\"]\n",
    "# valid_scales = cluster_aggregation_outs[\"valid_scales\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c5182",
   "metadata": {},
   "source": [
    "### POST-PROCESSING OF CLUSTERING RESULTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa77a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Do some post-processing on the clustering results\n",
    "X, Y, label_grid = create_2d_grid(\n",
    "    x=df[\"x\"].to_numpy(), y=df[\"y\"].to_numpy(), labels=cluster_pred, grid_spacing=None\n",
    ")\n",
    "\n",
    "# Remove small holes\n",
    "min_size = 50  # Minimum size of connected components to keep\n",
    "connectivity = 8  # 4 or 8 for pixel connectivity\n",
    "label_grid = remove_small_grid_components(\n",
    "    label_grid, min_size=min_size, connectivity=connectivity\n",
    ")\n",
    "\n",
    "# # Separate non-connected regions with same label\n",
    "# label_grid, label_mapping = split_disconnected_components(\n",
    "#     label_grid, connectivity=connectivity, start_label=0\n",
    "# )\n",
    "\n",
    "point_labels_cleaned, x, y = map_grid_to_points(\n",
    "    X,\n",
    "    Y,\n",
    "    label_grid,\n",
    "    x_points=df[\"x\"].to_numpy(),\n",
    "    y_points=df[\"y\"].to_numpy(),\n",
    "    keep_nan=True,\n",
    ")\n",
    "\n",
    "# === Compute similarity scores with prior clusters\n",
    "# Create a \"prior class\" assignment based on the sector with highest probability\n",
    "sector_names = list(PRIOR_PROBABILITY.keys())\n",
    "sector_assignments = np.zeros_like(cluster_pred)\n",
    "for i, point_probs in enumerate(prior_probs):\n",
    "    sector_assignments[i] = np.argmax(point_probs)\n",
    "\n",
    "# Compute similarity metrics\n",
    "ari = adjusted_rand_score(sector_assignments, cluster_pred)\n",
    "ami = adjusted_mutual_info_score(sector_assignments, cluster_pred)\n",
    "\n",
    "# === Make final clustering plot after cleaning\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.imshow(img, alpha=0.5, cmap=\"gray\")\n",
    "colormap = plt.get_cmap(\"tab10\")\n",
    "for i, label in enumerate(np.unique(point_labels_cleaned)):\n",
    "    mask = point_labels_cleaned == label\n",
    "    ax.scatter(\n",
    "        x[mask],\n",
    "        y[mask],\n",
    "        color=colormap(i),\n",
    "        label=f\"Cluster {label}\",\n",
    "        s=10,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "ax.legend(loc=\"upper right\", framealpha=0.9, fontsize=10)\n",
    "ax.set_aspect(\"equal\")\n",
    "\n",
    "\n",
    "if valid_scales is not None and len(valid_scales) > 1:\n",
    "    title = f\"Combined Clustering (scales: {valid_scales}, stability: {stability_score if stability_score is not None else '':.2f})\\nPrior Agreement: AMI={ami if ami is not None else 0.0:.2f}\"\n",
    "else:\n",
    "    title = f\"Clustering (scale: {sigma_values[0]})\\nPrior Agreement: AMI={ami if ami is not None else 0.0:.2f}\"\n",
    "\n",
    "ax.set_title(title)\n",
    "plt.savefig(\n",
    "    output_dir\n",
    "    / f\"{reference_start_date}_{reference_end_date}_kinematic_clustering.png\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "# plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded8f359",
   "metadata": {},
   "source": [
    "## FIND DISCONTINUITY IN VELOCITY FIELD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa3175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppcluster.discontinuity import (\n",
    "    find_vertical_discontinuities,\n",
    "    plot_discontinuities,\n",
    ")\n",
    "\n",
    "df_smooth = apply_2d_gaussian_filter(df, sigma=0)\n",
    "x = df_smooth[\"x\"].to_numpy()\n",
    "y = df_smooth[\"y\"].to_numpy()\n",
    "v = df_smooth[\"V\"].to_numpy()\n",
    "\n",
    "discontinuity_results = find_vertical_discontinuities(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    v=v,\n",
    "    vertical_bins=50,\n",
    "    horizontal_bins=10,\n",
    "    min_points_per_bin_col=20,  # try 5-20 depending on data density\n",
    "    gradient_threshold_factor=0.3,  # # Threshold for significant gradient (as fraction of max) adjust to be more/less sensitive\n",
    "    smoothing_sigma_1d=1.0,\n",
    "    min_strength=1e-3,\n",
    "    cluster_eps_factor=2.0,\n",
    "    cluster_min_samples=3,\n",
    "    border=[500, 500, 1000, 500],  # left, right, bottom, top in px units\n",
    "    sectors=sectors,  # optional: use predefined sectors instead of DBSCAN\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_discontinuities(\n",
    "    x,\n",
    "    y,\n",
    "    v,\n",
    "    discontinuities=discontinuity_results,\n",
    "    img=img,\n",
    "    ax=ax,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f52f47",
   "metadata": {},
   "source": [
    "## Morpho-kinematic analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe487b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppcluster.mcmc.postproc import (\n",
    "    create_2d_grid,\n",
    "    map_grid_to_points,\n",
    "    split_disconnected_components,\n",
    ")\n",
    "\n",
    "# Retrieve data\n",
    "kinematics_cluster = point_labels_cleaned.copy()\n",
    "\n",
    "df_smooth = apply_2d_gaussian_filter(df, sigma=1)\n",
    "x = df_smooth[\"x\"].to_numpy()\n",
    "y = df_smooth[\"y\"].to_numpy()\n",
    "v = df_smooth[\"V\"].to_numpy()\n",
    "\n",
    "# Ensure arrays are numpy arrays\n",
    "x = np.asarray(x)\n",
    "y = np.asarray(y)\n",
    "kin_cluster = np.asarray(kinematics_cluster)\n",
    "\n",
    "X, Y, kin_cluster_grid = create_2d_grid(x=x, y=y, labels=kin_cluster)\n",
    "\n",
    "# Filter out small clusters\n",
    "kin_cluster_grid = remove_small_grid_components(\n",
    "    kin_cluster_grid, min_size=100, connectivity=8\n",
    ")\n",
    "\n",
    "# Split clusters along detected discontinuities\n",
    "kin_cluster_grid, split_mapping = split_disconnected_components(\n",
    "    kin_cluster_grid, connectivity=8, start_label=0\n",
    ")\n",
    "kin_cluster, x, y = map_grid_to_points(X, Y, kin_cluster_grid, x, y, keep_nan=True)\n",
    "\n",
    "# Remove non classified points (-1 label)\n",
    "valid_mask = kin_cluster >= 0\n",
    "x = x[valid_mask]\n",
    "y = y[valid_mask]\n",
    "v = v[valid_mask]\n",
    "kin_cluster = kin_cluster[valid_mask]\n",
    "\n",
    "# Order clusters by median y descending (bottom = largest y first)\n",
    "clusters_ids = np.unique(kin_cluster)\n",
    "cluster_median_y = {int(c): float(np.median(y[kin_cluster == c])) for c in clusters_ids}\n",
    "\n",
    "ordered_clusters_ids = sorted(\n",
    "    clusters_ids, key=lambda c: cluster_median_y[int(c)], reverse=True\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.imshow(img, alpha=0.5, cmap=\"gray\")\n",
    "colormap = plt.get_cmap(\"tab10\")\n",
    "for i, label in enumerate(ordered_clusters_ids):\n",
    "    mask = kin_cluster == label\n",
    "    ax.scatter(\n",
    "        x[mask],\n",
    "        y[mask],\n",
    "        color=colormap(i),\n",
    "        label=f\"Cluster {label}\",\n",
    "        s=10,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "ax.legend(loc=\"upper right\", framealpha=0.9, fontsize=10)\n",
    "ax.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a731e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual assignment of clusters to sectors\n",
    "mk_label_str = np.full_like(kin_cluster, \"\", dtype=object)\n",
    "mk_label_id = -1 * np.ones_like(kin_cluster, dtype=int)\n",
    "\n",
    "# Assign cluster 1 to sector A\n",
    "mk_label_str[kin_cluster == ordered_clusters_ids[0]] = \"A\"\n",
    "mk_label_id[kin_cluster == ordered_clusters_ids[0]] = 0\n",
    "\n",
    "# Assign cluster 2 to sector B\n",
    "mk_label_str[kin_cluster == ordered_clusters_ids[1]] = \"B\"\n",
    "mk_label_id[kin_cluster == ordered_clusters_ids[1]] = 1\n",
    "\n",
    "# Assign cluster 0 to sector B1 (fast area in sector B)\n",
    "# mk_label_str[kin_cluster == ordered_clusters_ids[2]] = \"B1\"\n",
    "# mk_label_id[kin_cluster == ordered_clusters_ids[2]] = 2\n",
    "\n",
    "# Assign cluster 4 to sector C\n",
    "mk_label_str[kin_cluster == ordered_clusters_ids[2]] = \"C\"\n",
    "mk_label_id[kin_cluster == ordered_clusters_ids[2]] = 2\n",
    "\n",
    "# Assign cluster 5 to sector D\n",
    "mk_label_str[kin_cluster == ordered_clusters_ids[3]] = \"D\"\n",
    "mk_label_id[kin_cluster == ordered_clusters_ids[3]] = 3\n",
    "\n",
    "# Assign cluster 3 to sector E (fast area in sector D)\n",
    "mk_label_str[kin_cluster == ordered_clusters_ids[4]] = \"D1\"\n",
    "mk_label_id[kin_cluster == ordered_clusters_ids[4]] = 4\n",
    "\n",
    "unique_mk, counts = np.unique(mk_label_str[mk_label_str != \"\"], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f8aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors: red, orange, dark_orange yellow, green, dark_green\n",
    "colors = {\n",
    "    \"A\": \"#b3140b\",\n",
    "    \"B\": \"#ee9c21\",\n",
    "    \"B1\": \"#ff4800\",\n",
    "    \"C\": \"#f1ee30\",\n",
    "    \"D\": \"#5fb61c\",\n",
    "    \"D1\": \"#006837\",\n",
    "}\n",
    "\n",
    "\n",
    "# Prepare output arrays (string labels like \"A1\",\"B1\",\"C1\", and numeric ids)\n",
    "print(\"Morpho-kinematic assignment summary:\")\n",
    "for u, c in zip(unique_mk, counts, strict=False):\n",
    "    print(f\"  {u}: {c} points\")\n",
    "\n",
    "# Plot the assignment for quick inspection\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "if img is not None:\n",
    "    ax.imshow(img, alpha=0.5, cmap=\"gray\")\n",
    "\n",
    "# Plot points colored by mk label\n",
    "for lab in unique_mk:\n",
    "    mask = mk_label_str == lab\n",
    "    ax.scatter(x[mask], y[mask], color=colors[lab], label=lab, s=10, alpha=0.8)\n",
    "\n",
    "ax.legend(loc=\"upper right\", fontsize=9)\n",
    "ax.set_title(\"Morpho-Kinematic Sectors\")\n",
    "ax.set_aspect(\"equal\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c6714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mk_sector_polygons(\n",
    "    x,\n",
    "    y,\n",
    "    mk_label_str,\n",
    "    *,\n",
    "    smooth_iters=2,\n",
    "    prevent_overlap=True,\n",
    "    containment_strategy=\"difference\",\n",
    "):\n",
    "    \"\"\"Return smoothed MK-sector polygons with optional overlap and containment handling.\"\"\"\n",
    "    import warnings\n",
    "    from collections import defaultdict\n",
    "\n",
    "    import numpy as np\n",
    "    from scipy.spatial import Delaunay\n",
    "\n",
    "    containment_strategy = str(containment_strategy).lower()\n",
    "    if containment_strategy not in {\"difference\", \"keep\"}:\n",
    "        raise ValueError(\"containment_strategy must be 'difference' or 'keep'.\")\n",
    "\n",
    "    shapely_available = False\n",
    "    Polygon = MultiPolygon = unary_union = None\n",
    "    require_shapely = prevent_overlap or containment_strategy == \"difference\"\n",
    "    if require_shapely:\n",
    "        try:\n",
    "            from shapely.geometry import MultiPolygon, Polygon  # type: ignore\n",
    "            from shapely.ops import unary_union  # type: ignore\n",
    "\n",
    "            shapely_available = True\n",
    "        except ImportError:\n",
    "            warnings.warn(\n",
    "                \"Shapely is not installed. Falling back to simple polygons without \"\n",
    "                \"overlap containment handling.\",\n",
    "                RuntimeWarning,\n",
    "            )\n",
    "            prevent_overlap = False\n",
    "            containment_strategy = \"keep\"\n",
    "\n",
    "    def _chaikin(poly, n_iter=2):\n",
    "        if poly is None or len(poly) < 3 or n_iter <= 0:\n",
    "            return poly\n",
    "        P = np.asarray(poly, float)\n",
    "        for _ in range(n_iter):\n",
    "            if not np.allclose(P[0], P[-1]):\n",
    "                P = np.vstack([P, P[0]])\n",
    "            Q = []\n",
    "            for i in range(len(P) - 1):\n",
    "                p0, p1 = P[i], P[i + 1]\n",
    "                Q.append(0.75 * p0 + 0.25 * p1)\n",
    "                Q.append(0.25 * p0 + 0.75 * p1)\n",
    "            P = np.array(Q)\n",
    "        if np.allclose(P[0], P[-1]):\n",
    "            P = P[:-1]\n",
    "        return P\n",
    "\n",
    "    def _boundary_polygon(points):\n",
    "        pts = np.asarray(points, float)\n",
    "        if pts.shape[0] < 3:\n",
    "            return None\n",
    "        try:\n",
    "            tri = Delaunay(pts)\n",
    "        except Exception:\n",
    "            return None\n",
    "        edge_count = defaultdict(int)\n",
    "        for simplex in tri.simplices:\n",
    "            simplex = list(simplex)\n",
    "            edges = [\n",
    "                (simplex[i], simplex[j]) for i in range(3) for j in range(i + 1, 3)\n",
    "            ]\n",
    "            for i, j in edges:\n",
    "                if i > j:\n",
    "                    i, j = j, i\n",
    "                edge_count[(i, j)] += 1\n",
    "        boundary_edges = [e for e, c in edge_count.items() if c == 1]\n",
    "        if not boundary_edges:\n",
    "            return None\n",
    "        adj = defaultdict(list)\n",
    "        for i, j in boundary_edges:\n",
    "            adj[i].append(j)\n",
    "            adj[j].append(i)\n",
    "        start = boundary_edges[0][0]\n",
    "        poly_idx = [start]\n",
    "        prev = None\n",
    "        current = start\n",
    "        while True:\n",
    "            neigh = adj[current]\n",
    "            nxt_candidates = [n for n in neigh if n != prev]\n",
    "            if not nxt_candidates:\n",
    "                break\n",
    "            nxt = nxt_candidates[0]\n",
    "            if nxt == poly_idx[0] and len(poly_idx) >= 3:\n",
    "                break\n",
    "            poly_idx.append(nxt)\n",
    "            prev, current = current, nxt\n",
    "            if len(poly_idx) > len(pts) * 2:\n",
    "                break\n",
    "        if len(poly_idx) < 3:\n",
    "            return None\n",
    "        return pts[poly_idx]\n",
    "\n",
    "    class PolygonDict(dict):\n",
    "        def __init__(self, *args, geometries=None, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.geometries = geometries or {}\n",
    "\n",
    "    labels = np.asarray(mk_label_str)\n",
    "    x = np.asarray(x, float)\n",
    "    y = np.asarray(y, float)\n",
    "    unique_labels = [lab for lab in np.unique(labels) if isinstance(lab, str) and lab]\n",
    "\n",
    "    polygons = PolygonDict()\n",
    "    geometries = {}\n",
    "\n",
    "    for lab in unique_labels:\n",
    "        mask = labels == lab\n",
    "        pts = np.column_stack([x[mask], y[mask]])\n",
    "        poly = _boundary_polygon(pts)\n",
    "        if poly is None:\n",
    "            continue\n",
    "        poly_sm = _chaikin(poly, n_iter=smooth_iters)\n",
    "        if poly_sm is None or len(poly_sm) < 3:\n",
    "            continue\n",
    "\n",
    "        poly_shape = None\n",
    "        if shapely_available:\n",
    "            poly_shape = Polygon(poly_sm)\n",
    "            if not poly_shape.is_valid:\n",
    "                poly_shape = poly_shape.buffer(0)\n",
    "\n",
    "        if shapely_available and poly_shape and not poly_shape.is_empty:\n",
    "            existing_union = (\n",
    "                unary_union(list(geometries.values())) if geometries else None\n",
    "            )\n",
    "            containing_labels = [\n",
    "                prev_lab\n",
    "                for prev_lab, prev_shape in geometries.items()\n",
    "                if prev_shape.contains(poly_shape)\n",
    "            ]\n",
    "\n",
    "            if containing_labels and containment_strategy == \"difference\":\n",
    "                for prev_lab in containing_labels:\n",
    "                    updated_shape = geometries[prev_lab].difference(poly_shape)\n",
    "                    if updated_shape.is_empty:\n",
    "                        geometries.pop(prev_lab, None)\n",
    "                        polygons.pop(prev_lab, None)\n",
    "                    else:\n",
    "                        if isinstance(updated_shape, MultiPolygon):\n",
    "                            updated_shape = max(\n",
    "                                updated_shape.geoms, key=lambda g: g.area\n",
    "                            )\n",
    "                        geometries[prev_lab] = updated_shape\n",
    "                        coords = np.asarray(updated_shape.exterior.coords)\n",
    "                        if coords.shape[0] >= 4:\n",
    "                            polygons[prev_lab] = coords[:-1]\n",
    "                        else:\n",
    "                            polygons.pop(prev_lab, None)\n",
    "                existing_union = (\n",
    "                    unary_union(list(geometries.values())) if geometries else None\n",
    "                )\n",
    "\n",
    "            skip_difference = bool(containing_labels and containment_strategy == \"keep\")\n",
    "\n",
    "            if prevent_overlap and not skip_difference and existing_union:\n",
    "                poly_shape = poly_shape.difference(existing_union)\n",
    "\n",
    "            if poly_shape.is_empty:\n",
    "                continue\n",
    "\n",
    "            if isinstance(poly_shape, MultiPolygon):\n",
    "                poly_shape = max(poly_shape.geoms, key=lambda g: g.area)\n",
    "            coords = np.asarray(poly_shape.exterior.coords)\n",
    "            if coords.shape[0] < 4:\n",
    "                continue\n",
    "            polygons[lab] = coords[:-1]\n",
    "            geometries[lab] = poly_shape\n",
    "        else:\n",
    "            polygons[lab] = np.asarray(poly_sm, float)\n",
    "\n",
    "    if shapely_available:\n",
    "        polygons.geometries = geometries\n",
    "\n",
    "    return polygons\n",
    "\n",
    "\n",
    "def plot_mk_sectors_smooth_perimeters(\n",
    "    x,\n",
    "    y,\n",
    "    mk_label_str,\n",
    "    *,\n",
    "    v=None,\n",
    "    img=None,\n",
    "    smooth_iters=2,\n",
    "    colormap=None,\n",
    "    palette=\"tab20\",\n",
    "    fill=False,\n",
    "    fill_alpha=0.2,\n",
    "    edge_alpha=0.95,\n",
    "    scatter_alpha=0.3,\n",
    "    ax=None,\n",
    "    polygons=None,\n",
    "    polygon_kwargs=None,\n",
    "):\n",
    "    \"\"\"Plot MK-sector perimeters, optionally reusing pre-computed polygons.\"\"\"\n",
    "    import numpy as np\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    labels = np.asarray(mk_label_str)\n",
    "    x = np.asarray(x, float)\n",
    "    y = np.asarray(y, float)\n",
    "\n",
    "    if polygons is None:\n",
    "        polygon_kwargs = polygon_kwargs or {}\n",
    "        polygons = compute_mk_sector_polygons(\n",
    "            x,\n",
    "            y,\n",
    "            labels,\n",
    "            smooth_iters=smooth_iters,\n",
    "            **polygon_kwargs,\n",
    "        )\n",
    "\n",
    "    unique_labels = list(polygons.keys())\n",
    "    if colormap is not None:\n",
    "        palette_colors = [colormap.get(lab, (0.5, 0.5, 0.5)) for lab in unique_labels]\n",
    "    else:\n",
    "        cmap = plt.get_cmap(palette)\n",
    "        palette_colors = cmap(np.linspace(0, 1, max(len(unique_labels), 1)))\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    else:\n",
    "        fig = ax.figure\n",
    "\n",
    "    if img is not None:\n",
    "        ax.imshow(img, cmap=\"gray\", alpha=0.5)\n",
    "\n",
    "    if v is not None:\n",
    "        sc = ax.scatter(\n",
    "            x,\n",
    "            y,\n",
    "            c=np.asarray(v, float),\n",
    "            cmap=\"viridis\",\n",
    "            s=4,\n",
    "            alpha=scatter_alpha,\n",
    "            zorder=0,\n",
    "        )\n",
    "        cbar = fig.colorbar(sc, ax=ax, fraction=0.046, pad=0.04)\n",
    "        cbar.set_label(\"Velocity\", rotation=270, labelpad=15)\n",
    "\n",
    "    for idx, lab in enumerate(unique_labels):\n",
    "        poly = polygons[lab]\n",
    "        if poly is None or len(poly) < 3:\n",
    "            continue\n",
    "        edge_color = (\n",
    "            colormap.get(lab, palette_colors[idx]) if colormap else palette_colors[idx]\n",
    "        )\n",
    "\n",
    "        if fill:\n",
    "            ax.fill(\n",
    "                poly[:, 0],\n",
    "                poly[:, 1],\n",
    "                color=edge_color,\n",
    "                alpha=fill_alpha,\n",
    "                lw=0,\n",
    "                zorder=1,\n",
    "            )\n",
    "\n",
    "        ax.plot(\n",
    "            np.r_[poly[:, 0], poly[0, 0]],\n",
    "            np.r_[poly[:, 1], poly[0, 1]],\n",
    "            color=edge_color,\n",
    "            alpha=edge_alpha,\n",
    "            lw=2,\n",
    "            label=lab,\n",
    "            zorder=2,\n",
    "        )\n",
    "\n",
    "        cx, cy = poly.mean(axis=0)\n",
    "        ax.text(\n",
    "            cx,\n",
    "            cy,\n",
    "            lab,\n",
    "            color=\"k\",\n",
    "            fontsize=10,\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            bbox=dict(facecolor=\"white\", alpha=0.6, edgecolor=\"none\", pad=1.5),\n",
    "            zorder=3,\n",
    "        )\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    if unique_labels:\n",
    "        ax.legend(loc=\"upper right\", fontsize=9, framealpha=0.85)\n",
    "    ax.set_title(\"Morpho-kinematic sectors\")\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def compute_mk_sector_stats(\n",
    "    polygons,\n",
    "    mk_label_str,\n",
    "    *,\n",
    "    x,\n",
    "    y,\n",
    "    v=None,\n",
    "    img_shape=None,\n",
    "    rasterize=False,\n",
    "):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    def _poly_area(poly):\n",
    "        x0, y0 = poly[:, 0], poly[:, 1]\n",
    "        return 0.5 * np.abs(np.dot(x0, np.roll(y0, -1)) - np.dot(y0, np.roll(x0, -1)))\n",
    "\n",
    "    def _poly_perimeter(poly):\n",
    "        diffs = np.diff(np.vstack([poly, poly[0]]), axis=0)\n",
    "        return np.sum(np.hypot(diffs[:, 0], diffs[:, 1]))\n",
    "\n",
    "    def _poly_centroid(poly):\n",
    "        x0, y0 = poly[:, 0], poly[:, 1]\n",
    "        a = np.dot(x0, np.roll(y0, -1)) - np.dot(y0, np.roll(x0, -1))\n",
    "        A = a / 2.0\n",
    "        if np.isclose(A, 0):\n",
    "            return np.array([x0.mean(), y0.mean()])\n",
    "        cx = (1 / (6 * A)) * np.sum(\n",
    "            (x0 + np.roll(x0, -1)) * (x0 * np.roll(y0, -1) - y0 * np.roll(x0, -1))\n",
    "        )\n",
    "        cy = (1 / (6 * A)) * np.sum(\n",
    "            (y0 + np.roll(y0, -1)) * (x0 * np.roll(y0, -1) - y0 * np.roll(x0, -1))\n",
    "        )\n",
    "        return np.array([cx, cy])\n",
    "\n",
    "    try:\n",
    "        from skimage.draw import polygon as sk_polygon\n",
    "\n",
    "        sk_ok = True\n",
    "    except Exception:\n",
    "        sk_ok = False\n",
    "\n",
    "    labels = np.asarray(mk_label_str)\n",
    "    x = np.asarray(x, float)\n",
    "    y = np.asarray(y, float)\n",
    "    v = np.asarray(v, float) if v is not None else None\n",
    "    stats_rows = []\n",
    "    for lab, poly in polygons.items():\n",
    "        if poly is None or len(poly) < 3:\n",
    "            continue\n",
    "        mask = labels == lab\n",
    "        n_pts = int(np.sum(mask))\n",
    "        area = _poly_area(poly)\n",
    "        perim = _poly_perimeter(poly)\n",
    "        centroid = _poly_centroid(poly)\n",
    "        compactness = (4 * np.pi * area) / (perim**2 + 1e-12) if perim > 0 else np.nan\n",
    "        vel_stats = dict(\n",
    "            v_mean=np.nan, v_std=np.nan, v_median=np.nan, v_min=np.nan, v_max=np.nan\n",
    "        )\n",
    "        if v is not None and n_pts > 0:\n",
    "            v_sel = v[mask]\n",
    "            vel_stats = dict(\n",
    "                v_mean=float(np.mean(v_sel)),\n",
    "                v_std=float(np.std(v_sel)),\n",
    "                v_median=float(np.median(v_sel)),\n",
    "                v_min=float(np.min(v_sel)),\n",
    "                v_max=float(np.max(v_sel)),\n",
    "            )\n",
    "        pixel_count = np.nan\n",
    "        if rasterize and sk_ok and img_shape is not None:\n",
    "            h, w = img_shape[:2]\n",
    "            rr, cc = sk_polygon(poly[:, 1], poly[:, 0], shape=(h, w))\n",
    "            pixel_count = int(len(rr))\n",
    "        stats_rows.append(\n",
    "            {\n",
    "                \"label\": lab,\n",
    "                \"n_points\": n_pts,\n",
    "                \"area_px2\": float(area),\n",
    "                \"perimeter_px\": float(perim),\n",
    "                \"compactness\": float(compactness),\n",
    "                \"centroid_x\": float(centroid[0]),\n",
    "                \"centroid_y\": float(centroid[1]),\n",
    "                \"pixel_count\": pixel_count,\n",
    "                \"point_density_pts_per_px2\": float(n_pts / area)\n",
    "                if area > 0\n",
    "                else np.nan,\n",
    "                **vel_stats,\n",
    "            }\n",
    "        )\n",
    "    columns = [\n",
    "        \"label\",\n",
    "        \"n_points\",\n",
    "        \"area_px2\",\n",
    "        \"perimeter_px\",\n",
    "        \"compactness\",\n",
    "        \"centroid_x\",\n",
    "        \"centroid_y\",\n",
    "        \"pixel_count\",\n",
    "        \"point_density_pts_per_px2\",\n",
    "        \"v_mean\",\n",
    "        \"v_std\",\n",
    "        \"v_median\",\n",
    "        \"v_min\",\n",
    "        \"v_max\",\n",
    "    ]\n",
    "    return (\n",
    "        pd.DataFrame(stats_rows, columns=columns)\n",
    "        .sort_values(\"label\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "colors = {\n",
    "    \"A\": \"#b3140b\",\n",
    "    \"B\": \"#ee9c21\",\n",
    "    \"B1\": \"#ff4800\",\n",
    "    \"C\": \"#f1ee30\",\n",
    "    \"D\": \"#5fb61c\",\n",
    "    \"D1\": \"#006837\",\n",
    "}\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot the main clusters - Exclude clusters B1 and D1 from filled plot\n",
    "minor_clusters = [\"B1\", \"D1\"]\n",
    "mask = np.isin(mk_label_str, minor_clusters)\n",
    "x_major = x[~mask]\n",
    "y_major = y[~mask]\n",
    "mk_label_str_major = mk_label_str[~mask]\n",
    "colors_plot = {k: v for k, v in colors.items() if k not in minor_clusters}\n",
    "polygons_major = compute_mk_sector_polygons(\n",
    "    x_major,\n",
    "    y_major,\n",
    "    mk_label_str_major,\n",
    "    smooth_iters=5,\n",
    "    prevent_overlap=True,\n",
    ")\n",
    "\n",
    "plot_mk_sectors_smooth_perimeters(\n",
    "    x_major,\n",
    "    y_major,\n",
    "    mk_label_str_major,\n",
    "    img=np.asarray(img),\n",
    "    smooth_iters=4,\n",
    "    colormap=colors_plot,\n",
    "    fill=True,\n",
    "    fill_alpha=0.1,\n",
    "    edge_alpha=1,\n",
    "    scatter_alpha=0.3,\n",
    "    polygons=polygons_major,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "\n",
    "# Add the excluded clusters as outlines only\n",
    "mask = np.isin(mk_label_str, minor_clusters)\n",
    "x_minor = x[mask]\n",
    "y_minor = y[mask]\n",
    "mk_label_str_minor = mk_label_str[mask]\n",
    "colors_plot = {k: v for k, v in colors.items() if k in minor_clusters}\n",
    "polygons_minor = compute_mk_sector_polygons(\n",
    "    x_minor,\n",
    "    y_minor,\n",
    "    mk_label_str_minor,\n",
    "    smooth_iters=4,\n",
    "    prevent_overlap=False,\n",
    ")\n",
    "plot_mk_sectors_smooth_perimeters(\n",
    "    x_minor,\n",
    "    y_minor,\n",
    "    mk_label_str_minor,\n",
    "    smooth_iters=2,\n",
    "    colormap=colors_plot,\n",
    "    fill=True,\n",
    "    fill_alpha=0.1,\n",
    "    edge_alpha=1,\n",
    "    scatter_alpha=0.3,\n",
    "    polygons=polygons_minor,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f062822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_mk_sector_polygons(\n",
    "#     x,\n",
    "#     y,\n",
    "#     mk_label_str,\n",
    "#     *,\n",
    "#     smooth_iters=2,\n",
    "#     prevent_overlap=True,\n",
    "#     containment_strategy=\"difference\",\n",
    "# ):\n",
    "#     \"\"\"Return smoothed MK-sector polygons with optional overlap and containment handling.\"\"\"\n",
    "#     import warnings\n",
    "#     from collections import defaultdict\n",
    "\n",
    "#     import numpy as np\n",
    "#     from scipy.spatial import Delaunay\n",
    "\n",
    "#     containment_strategy = str(containment_strategy).lower()\n",
    "#     if containment_strategy not in {\"difference\", \"keep\"}:\n",
    "#         raise ValueError(\"containment_strategy must be 'difference' or 'keep'.\")\n",
    "\n",
    "#     shapely_available = False\n",
    "#     Polygon = MultiPolygon = unary_union = None\n",
    "#     require_shapely = prevent_overlap or containment_strategy == \"difference\"\n",
    "#     if require_shapely:\n",
    "#         try:\n",
    "#             from shapely.geometry import MultiPolygon, Polygon  # type: ignore\n",
    "#             from shapely.ops import unary_union  # type: ignore\n",
    "\n",
    "#             shapely_available = True\n",
    "#         except ImportError:\n",
    "#             warnings.warn(\n",
    "#                 \"Shapely is not installed. Falling back to simple polygons without \"\n",
    "#                 \"overlap/containment handling.\",\n",
    "#                 RuntimeWarning,\n",
    "#             )\n",
    "#             prevent_overlap = False\n",
    "#             containment_strategy = \"keep\"\n",
    "\n",
    "#     def _chaikin(poly, n_iter=2):\n",
    "#         if poly is None or len(poly) < 3 or n_iter <= 0:\n",
    "#             return poly\n",
    "#         P = np.asarray(poly, float)\n",
    "#         for _ in range(n_iter):\n",
    "#             if not np.allclose(P[0], P[-1]):\n",
    "#                 P = np.vstack([P, P[0]])\n",
    "#             Q = []\n",
    "#             for i in range(len(P) - 1):\n",
    "#                 p0, p1 = P[i], P[i + 1]\n",
    "#                 Q.append(0.75 * p0 + 0.25 * p1)\n",
    "#                 Q.append(0.25 * p0 + 0.75 * p1)\n",
    "#             P = np.array(Q)\n",
    "#         if np.allclose(P[0], P[-1]):\n",
    "#             P = P[:-1]\n",
    "#         return P\n",
    "\n",
    "#     def _boundary_polygon(points):\n",
    "#         pts = np.asarray(points, float)\n",
    "#         if pts.shape[0] < 3:\n",
    "#             return None\n",
    "#         try:\n",
    "#             tri = Delaunay(pts)\n",
    "#         except Exception:\n",
    "#             return None\n",
    "#         edge_count = defaultdict(int)\n",
    "#         for simplex in tri.simplices:\n",
    "#             simplex = list(simplex)\n",
    "#             edges = [\n",
    "#                 (simplex[i], simplex[j]) for i in range(3) for j in range(i + 1, 3)\n",
    "#             ]\n",
    "#             for i, j in edges:\n",
    "#                 if i > j:\n",
    "#                     i, j = j, i\n",
    "#                 edge_count[(i, j)] += 1\n",
    "#         boundary_edges = [e for e, c in edge_count.items() if c == 1]\n",
    "#         if not boundary_edges:\n",
    "#             return None\n",
    "#         adj = defaultdict(list)\n",
    "#         for i, j in boundary_edges:\n",
    "#             adj[i].append(j)\n",
    "#             adj[j].append(i)\n",
    "#         start = boundary_edges[0][0]\n",
    "#         poly_idx = [start]\n",
    "#         prev = None\n",
    "#         current = start\n",
    "#         while True:\n",
    "#             neigh = adj[current]\n",
    "#             nxt_candidates = [n for n in neigh if n != prev]\n",
    "#             if not nxt_candidates:\n",
    "#                 break\n",
    "#             nxt = nxt_candidates[0]\n",
    "#             if nxt == poly_idx[0] and len(poly_idx) >= 3:\n",
    "#                 break\n",
    "#             poly_idx.append(nxt)\n",
    "#             prev, current = current, nxt\n",
    "#             if len(poly_idx) > len(pts) * 2:\n",
    "#                 break\n",
    "#         if len(poly_idx) < 3:\n",
    "#             return None\n",
    "#         return pts[poly_idx]\n",
    "\n",
    "#     labels = np.asarray(mk_label_str)\n",
    "#     x = np.asarray(x, float)\n",
    "#     y = np.asarray(y, float)\n",
    "#     unique_labels = [lab for lab in np.unique(labels) if isinstance(lab, str) and lab]\n",
    "\n",
    "#     polygons: dict[str, np.ndarray] = {}\n",
    "#     geometries: dict[str, \"Polygon\"] = {}\n",
    "\n",
    "#     for lab in unique_labels:\n",
    "#         mask = labels == lab\n",
    "#         pts = np.column_stack([x[mask], y[mask]])\n",
    "#         poly = _boundary_polygon(pts)\n",
    "#         if poly is None:\n",
    "#             continue\n",
    "#         poly_sm = _chaikin(poly, n_iter=smooth_iters)\n",
    "#         if poly_sm is None or len(poly_sm) < 3:\n",
    "#             continue\n",
    "\n",
    "#         poly_shape = None\n",
    "#         if shapely_available:\n",
    "#             poly_shape = Polygon(poly_sm)\n",
    "#             if not poly_shape.is_valid:\n",
    "#                 poly_shape = poly_shape.buffer(0)\n",
    "\n",
    "#         if shapely_available and poly_shape and not poly_shape.is_empty:\n",
    "#             containing_labels = [\n",
    "#                 prev_lab\n",
    "#                 for prev_lab, prev_shape in geometries.items()\n",
    "#                 if prev_shape.contains(poly_shape)\n",
    "#             ]\n",
    "#             contained_labels = [\n",
    "#                 prev_lab\n",
    "#                 for prev_lab, prev_shape in geometries.items()\n",
    "#                 if poly_shape.contains(prev_shape)\n",
    "#             ]\n",
    "\n",
    "#             if containment_strategy == \"difference\":\n",
    "#                 for prev_lab in containing_labels:\n",
    "#                     updated_shape = geometries[prev_lab].difference(poly_shape)\n",
    "#                     if updated_shape.is_empty:\n",
    "#                         geometries.pop(prev_lab, None)\n",
    "#                         polygons.pop(prev_lab, None)\n",
    "#                         continue\n",
    "#                     if isinstance(updated_shape, MultiPolygon):\n",
    "#                         updated_shape = max(updated_shape.geoms, key=lambda g: g.area)\n",
    "#                     coords_prev = np.asarray(updated_shape.exterior.coords)\n",
    "#                     if coords_prev.shape[0] < 4:\n",
    "#                         geometries.pop(prev_lab, None)\n",
    "#                         polygons.pop(prev_lab, None)\n",
    "#                     else:\n",
    "#                         geometries[prev_lab] = updated_shape\n",
    "#                         polygons[prev_lab] = coords_prev[:-1]\n",
    "\n",
    "#                 if contained_labels:\n",
    "#                     subtract_union = unary_union(\n",
    "#                         [geometries[prev_lab] for prev_lab in contained_labels]\n",
    "#                     )\n",
    "#                     poly_shape = poly_shape.difference(subtract_union)\n",
    "#                     if poly_shape.is_empty:\n",
    "#                         continue\n",
    "#                     if isinstance(poly_shape, MultiPolygon):\n",
    "#                         poly_shape = max(poly_shape.geoms, key=lambda g: g.area)\n",
    "\n",
    "#             if prevent_overlap:\n",
    "#                 excluded_labels = set()\n",
    "#                 if containment_strategy == \"keep\":\n",
    "#                     excluded_labels.update(containing_labels)\n",
    "#                     excluded_labels.update(contained_labels)\n",
    "#                 overlap_targets = [\n",
    "#                     prev_shape\n",
    "#                     for prev_lab, prev_shape in geometries.items()\n",
    "#                     if prev_lab not in excluded_labels\n",
    "#                 ]\n",
    "#                 if overlap_targets:\n",
    "#                     overlap_union = unary_union(overlap_targets)\n",
    "#                     poly_shape = poly_shape.difference(overlap_union)\n",
    "#                     if poly_shape.is_empty:\n",
    "#                         continue\n",
    "#                     if isinstance(poly_shape, MultiPolygon):\n",
    "#                         poly_shape = max(poly_shape.geoms, key=lambda g: g.area)\n",
    "\n",
    "#             coords = np.asarray(poly_shape.exterior.coords)\n",
    "#             if coords.shape[0] < 4:\n",
    "#                 continue\n",
    "#             polygons[lab] = coords[:-1]\n",
    "#             geometries[lab] = poly_shape\n",
    "#         else:\n",
    "#             polygons[lab] = np.asarray(poly_sm, float)\n",
    "\n",
    "#     return polygons\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(8, 8))\n",
    "# polygons = compute_mk_sector_polygons(\n",
    "#     x,\n",
    "#     y,\n",
    "#     mk_label_str,\n",
    "#     smooth_iters=5,\n",
    "#     prevent_overlap=True,\n",
    "#     containment_strategy=\"difference\",\n",
    "# )\n",
    "\n",
    "# plot_mk_sectors_smooth_perimeters(\n",
    "#     x,\n",
    "#     y,\n",
    "#     mk_label_str,\n",
    "#     img=np.asarray(img),\n",
    "#     smooth_iters=4,\n",
    "#     colormap=colors_plot,\n",
    "#     fill=True,\n",
    "#     fill_alpha=0.1,\n",
    "#     edge_alpha=1,\n",
    "#     scatter_alpha=0.3,\n",
    "#     polygons=polygons,\n",
    "#     ax=ax,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e14f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_stats = compute_mk_sector_stats(\n",
    "    polygons_major,\n",
    "    mk_label_str,\n",
    "    x=x,\n",
    "    y=y,\n",
    "    v=v,\n",
    "    img_shape=np.asarray(img).shape if img is not None else None,\n",
    "    rasterize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c74ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f9d995",
   "metadata": {},
   "source": [
    "## Deprecated code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05402f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Morpho-kinematic assignment ---\n",
    "\n",
    "\n",
    "# kinematics_cluster = point_labels_cleaned\n",
    "\n",
    "# # Ensure arrays are numpy arrays\n",
    "# x = np.asarray(x)\n",
    "# y = np.asarray(y)\n",
    "# kin_cluster = np.asarray(kinematics_cluster)\n",
    "\n",
    "# # Remove non classified points (-1 label)\n",
    "# valid_mask = kin_cluster >= 0\n",
    "# x = x[valid_mask]\n",
    "# y = y[valid_mask]\n",
    "# kin_cluster = kin_cluster[valid_mask]\n",
    "\n",
    "# # Compute per-cluster median y (to order clusters from bottom -> top)\n",
    "# clusters = np.unique(kin_cluster)\n",
    "# cluster_median_y = {int(c): float(np.median(y[kin_cluster == c])) for c in clusters}\n",
    "\n",
    "# # Order clusters by median y descending (bottom = largest y first)\n",
    "# ordered_clusters = sorted(\n",
    "#     clusters, key=lambda c: cluster_median_y[int(c)], reverse=True\n",
    "# )\n",
    "\n",
    "# # Read discontinuities (clustered boundaries) from detection, if available\n",
    "# clustered_disc = (\n",
    "#     discontinuity_results.get(\"clustered\", [])\n",
    "#     if discontinuity_results is not None\n",
    "#     else []\n",
    "# )\n",
    "\n",
    "# logger.info(\"Found %d morphological discontinuities\", len(clustered_disc))\n",
    "\n",
    "\n",
    "# # If a discontinuity falls inside a kinematic cluster, split that cluster into two parts.\n",
    "\n",
    "\n",
    "# # replace the call to _split_clusters_on_discontinuities accordingly where used:\n",
    "# # previously: kin_cluster, splits = _split_clusters_on_discontinuities(kin_cluster, y, clustered_disc, min_points_side=20)\n",
    "# # now provide both x and y arrays:\n",
    "\n",
    "\n",
    "# # Perform splitting using detected discontinuities\n",
    "# if clustered_disc:\n",
    "#     kin_cluster, splits = _split_clusters_on_discontinuities(\n",
    "#         kin_cluster, x, y, clustered_disc, min_points_side=50\n",
    "#     )\n",
    "\n",
    "#     if splits:\n",
    "#         # recompute clusters and medians after splits\n",
    "#         clusters = np.unique(kin_cluster)\n",
    "#         cluster_median_y = {\n",
    "#             int(c): float(np.median(y[kin_cluster == c])) for c in clusters\n",
    "#         }\n",
    "#         ordered_clusters = sorted(\n",
    "#             clusters, key=lambda c: cluster_median_y[int(c)], reverse=True\n",
    "#         )\n",
    "#         logger.info(\"Performed %d cluster split(s) due to discontinuities\", len(splits))\n",
    "#     else:\n",
    "#         logger.info(\n",
    "#             \"No cluster splits performed (no discontinuity inside single cluster with enough support)\"\n",
    "#         )\n",
    "\n",
    "# # Read first discontinuity (the one separating bottom A from the rest), if available\n",
    "# first_discont_y = None\n",
    "# if clustered_disc:\n",
    "#     # clustered_boundaries were sorted reverse=True in detection, so first is bottommost discontinuity\n",
    "#     first_discont_y = float(clustered_disc[0][\"position\"])\n",
    "\n",
    "# # Prepare output arrays (string labels like \"A1\",\"B1\",\"C1\", and numeric ids)\n",
    "# mk_label_str = np.full_like(kin_cluster, \"\", dtype=object)\n",
    "# mk_label_id = -1 * np.ones_like(kin_cluster, dtype=int)\n",
    "\n",
    "# # Helper to assign a label for a given cluster and mask\n",
    "# next_mk_id = 0\n",
    "\n",
    "\n",
    "# def _assign(cluster_val, mask, label_text):\n",
    "#     global next_mk_id\n",
    "#     mk_label_str[mask] = label_text\n",
    "#     mk_label_id[mask] = next_mk_id\n",
    "#     next_mk_id += 1\n",
    "\n",
    "\n",
    "# # If we have a discontinuity, use it to split bottom vs above\n",
    "# if first_discont_y is not None:\n",
    "#     # Bottom region: y >= first_discont_y (image coords: large y -> bottom)\n",
    "#     bottom_mask = y >= first_discont_y\n",
    "#     clusters_in_bottom = (\n",
    "#         np.unique(kin_cluster[bottom_mask]) if np.any(bottom_mask) else np.array([])\n",
    "#     )\n",
    "\n",
    "#     # Order them bottom->up (by median y) and assign A1, A2, ...\n",
    "#     clusters_in_bottom_ordered = sorted(\n",
    "#         clusters_in_bottom, key=lambda c: cluster_median_y[int(c)], reverse=True\n",
    "#     )\n",
    "#     for i, cl in enumerate(clusters_in_bottom_ordered, start=1):\n",
    "#         mask = (kin_cluster == cl) & bottom_mask\n",
    "#         if not np.any(mask):\n",
    "#             continue\n",
    "#         _assign(int(cl), mask, f\"A{i}\")\n",
    "\n",
    "#     # Above region: y < first_discont_y\n",
    "#     above_mask = y < first_discont_y\n",
    "#     clusters_above = (\n",
    "#         np.unique(kin_cluster[above_mask]) if np.any(above_mask) else np.array([])\n",
    "#     )\n",
    "\n",
    "#     if clusters_above.size > 0:\n",
    "#         # Order by median y (closest to discontinuity first)\n",
    "#         clusters_above_ordered = sorted(\n",
    "#             clusters_above, key=lambda c: cluster_median_y[int(c)], reverse=True\n",
    "#         )\n",
    "\n",
    "#         # Assign B1 to the cluster closest to the discontinuity (highest median y among above)\n",
    "#         b_cl = clusters_above_ordered[0]\n",
    "#         b_mask = (kin_cluster == b_cl) & above_mask\n",
    "#         _assign(int(b_cl), b_mask, \"B1\")\n",
    "\n",
    "#         # Remaining above clusters -> assign to C1, C2, ... (upper part / slowest)\n",
    "#         for j, cl in enumerate(clusters_above_ordered[1:], start=1):\n",
    "#             mask = (kin_cluster == cl) & above_mask\n",
    "#             if not np.any(mask):\n",
    "#                 continue\n",
    "#             _assign(int(cl), mask, f\"C{j}\")\n",
    "# else:\n",
    "#     # No discontinuity found: use kinematic ordering to define A / B / C\n",
    "#     if len(ordered_clusters) == 1:\n",
    "#         # single cluster -> all A1\n",
    "#         _assign(int(ordered_clusters[0]), np.ones_like(kin_cluster, dtype=bool), \"A1\")\n",
    "#     elif len(ordered_clusters) == 2:\n",
    "#         # bottom -> A1, top -> B1\n",
    "#         _assign(int(ordered_clusters[0]), kin_cluster == ordered_clusters[0], \"A1\")\n",
    "#         _assign(int(ordered_clusters[1]), kin_cluster == ordered_clusters[1], \"B1\")\n",
    "#     else:\n",
    "#         # >=3 clusters: bottom->A1, next->B1, rest -> C1,C2...\n",
    "#         _assign(int(ordered_clusters[0]), kin_cluster == ordered_clusters[0], \"A1\")\n",
    "#         _assign(int(ordered_clusters[1]), kin_cluster == ordered_clusters[1], \"B1\")\n",
    "#         for j, cl in enumerate(ordered_clusters[2:], start=1):\n",
    "#             _assign(int(cl), kin_cluster == cl, f\"C{j}\")\n",
    "\n",
    "# # Summary counts\n",
    "# unique_mk, counts = np.unique(mk_label_str[mk_label_str != \"\"], return_counts=True)\n",
    "# print(\"Morpho-kinematic assignment summary:\")\n",
    "# for u, c in zip(unique_mk, counts, strict=False):\n",
    "#     print(f\"  {u}: {c} points\")\n",
    "\n",
    "# # Plot the assignment for quick inspection\n",
    "# fig, ax = plt.subplots(figsize=(8, 8))\n",
    "# if img is not None:\n",
    "#     ax.imshow(img, alpha=0.5, cmap=\"gray\")\n",
    "\n",
    "# # Create color map for labels\n",
    "# labels_present = list(unique_mk)\n",
    "# cmap = plt.get_cmap(\"tab10\")\n",
    "# colors = {lab: cmap(i % 10) for i, lab in enumerate(labels_present)}\n",
    "\n",
    "# # Plot points colored by mk label\n",
    "# for lab in labels_present:\n",
    "#     mask = mk_label_str == lab\n",
    "#     ax.scatter(x[mask], y[mask], color=colors[lab], label=lab, s=10, alpha=0.8)\n",
    "\n",
    "# # Plot kinematic cluster boundaries (optional: scatter of kinematic clusters)\n",
    "# # Also overlay discontinuities if present\n",
    "# if first_discont_y is not None:\n",
    "#     ax.axhline(\n",
    "#         first_discont_y,\n",
    "#         color=\"red\",\n",
    "#         linestyle=\"--\",\n",
    "#         linewidth=2,\n",
    "#         label=\"first discontinuity\",\n",
    "#     )\n",
    "\n",
    "# ax.legend(loc=\"upper right\", fontsize=9)\n",
    "# ax.set_title(\"Morpho-Kinematic Assignment\")\n",
    "# ax.set_aspect(\"equal\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33341fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Morpho-kinematic assignment ---\n",
    "\n",
    "# kinematics_cluster = point_labels_cleaned\n",
    "\n",
    "# # Ensure arrays are numpy arrays\n",
    "# x = np.asarray(x)\n",
    "# y = np.asarray(y)\n",
    "# kin_cluster = np.asarray(kinematics_cluster)\n",
    "\n",
    "# # Remove non classified points (-1 label)\n",
    "# valid_mask = kin_cluster >= 0\n",
    "# x = x[valid_mask]\n",
    "# y = y[valid_mask]\n",
    "# kin_cluster = kin_cluster[valid_mask]\n",
    "\n",
    "# # Compute per-cluster median y (to order clusters from bottom -> top)\n",
    "# clusters = np.unique(kin_cluster)\n",
    "# cluster_median_y = {int(c): float(np.median(y[kin_cluster == c])) for c in clusters}\n",
    "\n",
    "# # Order clusters by median y descending (bottom = largest y first)\n",
    "# ordered_clusters = sorted(\n",
    "#     clusters, key=lambda c: cluster_median_y[int(c)], reverse=True\n",
    "# )\n",
    "\n",
    "# # Read first discontinuity (the one separating bottom A from the rest), if available\n",
    "# first_discont_y = None\n",
    "# clustered_disc = (\n",
    "#     discontinuity_results.get(\"clustered\", [])\n",
    "#     if discontinuity_results is not None\n",
    "#     else []\n",
    "# )\n",
    "# if clustered_disc:\n",
    "#     # clustered_boundaries were sorted reverse=True in detection, so first is bottommost discontinuity\n",
    "#     first_discont_y = float(clustered_disc[0][\"position\"])\n",
    "\n",
    "# # Prepare output arrays (string labels like \"A1\",\"B1\",\"C1\", and numeric ids)\n",
    "# mk_label_str = np.full_like(kin_cluster, \"\", dtype=object)\n",
    "# mk_label_id = -1 * np.ones_like(kin_cluster, dtype=int)\n",
    "\n",
    "# # Helper to assign a label for a given cluster and mask\n",
    "# next_mk_id = 0\n",
    "\n",
    "\n",
    "# def _assign(cluster_val, mask, label_text):\n",
    "#     global next_mk_id\n",
    "#     mk_label_str[mask] = label_text\n",
    "#     mk_label_id[mask] = next_mk_id\n",
    "#     next_mk_id += 1\n",
    "\n",
    "\n",
    "# # If we have a discontinuity, use it to split bottom vs above\n",
    "# if first_discont_y is not None:\n",
    "#     # Bottom region: y >= first_discont_y (image coords: large y -> bottom)\n",
    "#     bottom_mask = y >= first_discont_y\n",
    "#     clusters_in_bottom = (\n",
    "#         np.unique(kin_cluster[bottom_mask]) if np.any(bottom_mask) else np.array([])\n",
    "#     )\n",
    "\n",
    "#     # Order them bottom->up (by median y) and assign A1, A2, ...\n",
    "#     clusters_in_bottom_ordered = sorted(\n",
    "#         clusters_in_bottom, key=lambda c: cluster_median_y[int(c)], reverse=True\n",
    "#     )\n",
    "#     for i, cl in enumerate(clusters_in_bottom_ordered, start=1):\n",
    "#         mask = (kin_cluster == cl) & bottom_mask\n",
    "#         if not np.any(mask):\n",
    "#             continue\n",
    "#         _assign(int(cl), mask, f\"A{i}\")\n",
    "\n",
    "#     # Above region: y < first_discont_y\n",
    "#     above_mask = y < first_discont_y\n",
    "#     clusters_above = (\n",
    "#         np.unique(kin_cluster[above_mask]) if np.any(above_mask) else np.array([])\n",
    "#     )\n",
    "\n",
    "#     if clusters_above.size > 0:\n",
    "#         # Order by median y (closest to discontinuity first)\n",
    "#         clusters_above_ordered = sorted(\n",
    "#             clusters_above, key=lambda c: cluster_median_y[int(c)], reverse=True\n",
    "#         )\n",
    "\n",
    "#         # Assign B1 to the cluster closest to the discontinuity (highest median y among above)\n",
    "#         b_cl = clusters_above_ordered[0]\n",
    "#         b_mask = (kin_cluster == b_cl) & above_mask\n",
    "#         _assign(int(b_cl), b_mask, \"B1\")\n",
    "\n",
    "#         # Remaining above clusters -> assign to C1, C2, ... (upper part / slowest)\n",
    "#         for j, cl in enumerate(clusters_above_ordered[1:], start=1):\n",
    "#             mask = (kin_cluster == cl) & above_mask\n",
    "#             if not np.any(mask):\n",
    "#                 continue\n",
    "#             _assign(int(cl), mask, f\"C{j}\")\n",
    "# else:\n",
    "#     # No discontinuity found: use kinematic ordering to define A / B / C\n",
    "#     if len(ordered_clusters) == 1:\n",
    "#         # single cluster -> all A1\n",
    "#         _assign(int(ordered_clusters[0]), np.ones_like(kin_cluster, dtype=bool), \"A1\")\n",
    "#     elif len(ordered_clusters) == 2:\n",
    "#         # bottom -> A1, top -> B1\n",
    "#         _assign(int(ordered_clusters[0]), kin_cluster == ordered_clusters[0], \"A1\")\n",
    "#         _assign(int(ordered_clusters[1]), kin_cluster == ordered_clusters[1], \"B1\")\n",
    "#     else:\n",
    "#         # >=3 clusters: bottom->A1, next->B1, rest -> C1,C2...\n",
    "#         _assign(int(ordered_clusters[0]), kin_cluster == ordered_clusters[0], \"A1\")\n",
    "#         _assign(int(ordered_clusters[1]), kin_cluster == ordered_clusters[1], \"B1\")\n",
    "#         for j, cl in enumerate(ordered_clusters[2:], start=1):\n",
    "#             _assign(int(cl), kin_cluster == cl, f\"C{j}\")\n",
    "\n",
    "# # Summary counts\n",
    "# unique_mk, counts = np.unique(mk_label_str[mk_label_str != \"\"], return_counts=True)\n",
    "# print(\"Morpho-kinematic assignment summary:\")\n",
    "# for u, c in zip(unique_mk, counts, strict=False):\n",
    "#     print(f\"  {u}: {c} points\")\n",
    "\n",
    "\n",
    "# # Plot the assignment for quick inspection\n",
    "# fig, ax = plt.subplots(figsize=(8, 8))\n",
    "# if img is not None:\n",
    "#     ax.imshow(img, alpha=0.5, cmap=\"gray\")\n",
    "\n",
    "# # Create color map for labels\n",
    "# labels_present = list(unique_mk)\n",
    "# cmap = plt.get_cmap(\"tab10\")\n",
    "# colors = {lab: cmap(i % 10) for i, lab in enumerate(labels_present)}\n",
    "\n",
    "# # Plot points colored by mk label\n",
    "# for lab in labels_present:\n",
    "#     mask = mk_label_str == lab\n",
    "#     ax.scatter(x[mask], y[mask], color=colors[lab], label=lab, s=10, alpha=0.8)\n",
    "\n",
    "# # Plot kinematic cluster boundaries (optional: scatter of kinematic clusters)\n",
    "# # Also overlay discontinuities if present\n",
    "# if first_discont_y is not None:\n",
    "#     ax.axhline(\n",
    "#         first_discont_y,\n",
    "#         color=\"red\",\n",
    "#         linestyle=\"--\",\n",
    "#         linewidth=2,\n",
    "#         label=\"first discontinuity\",\n",
    "#     )\n",
    "\n",
    "# ax.legend(loc=\"upper right\", fontsize=9)\n",
    "# ax.set_title(\"Morpho-Kinematic Assignment\")\n",
    "# ax.set_aspect(\"equal\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppcx-domains (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
