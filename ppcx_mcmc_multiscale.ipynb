{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "8e564560",
            "metadata": {},
            "outputs": [],
            "source": [
                "%reload_ext autoreload\n",
                "%autoreload 2\n",
                "%matplotlib widget\n",
                "\n",
                "from datetime import datetime\n",
                "from pathlib import Path\n",
                "\n",
                "import arviz as az\n",
                "import joblib\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import pymc as pm\n",
                "from matplotlib import colors as mcolors\n",
                "from matplotlib import pyplot as plt\n",
                "from matplotlib.colors import Normalize\n",
                "from sklearn.metrics import (\n",
                "    adjusted_mutual_info_score,\n",
                "    adjusted_rand_score,\n",
                ")\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sqlalchemy import create_engine\n",
                "\n",
                "from ppcluster import logger, mcmc\n",
                "from ppcluster.cvat import (\n",
                "    filter_dataframe_by_polygons,\n",
                "    read_polygons_from_cvat,\n",
                ")\n",
                "from ppcluster.griddata import create_2d_grid, map_grid_to_points\n",
                "from ppcluster.mcmc.postproc import (\n",
                "    aggregate_multiscale_clustering,\n",
                "    remove_small_grid_components,\n",
                "    split_disconnected_components,\n",
                ")\n",
                "from ppcluster.mksectors import (\n",
                "    auto_assign_mk_sectors,\n",
                "    compute_mk_sector_stats,\n",
                "    draw_polygon,\n",
                ")\n",
                "from ppcluster.preprocessing import (\n",
                "    apply_2d_gaussian_filter,\n",
                "    apply_dic_filters,\n",
                "    preprocess_velocity_features,\n",
                "    spatial_subsample,\n",
                ")\n",
                "from ppcluster.utils.config import ConfigManager\n",
                "from ppcluster.utils.database import (\n",
                "    fetch_dic_analysis_ids,\n",
                "    get_dic_analysis_by_ids,\n",
                "    get_image,\n",
                "    get_multi_dic_data,\n",
                ")\n",
                "\n",
                "RANDOM_SEED = 8927\n",
                "rng = np.random.default_rng(RANDOM_SEED)\n",
                "\n",
                "# Load configuration\n",
                "config = ConfigManager()\n",
                "db_engine = create_engine(config.db_url)\n",
                "\n",
                "\n",
                "SAVE_OUTPUTS = True  # Set to True to save inference results\n",
                "LOAD_EXISTING = False  # Set to False to run sampling again\n",
                "\n",
                "# MCMC parameters\n",
                "DRAWS = 2000  # Number of MCMC draws\n",
                "TUNE = 1000  # Number of tuning steps\n",
                "CHAINS = 4  # Number of MCMC chains\n",
                "CORES = 4  # Number of CPU cores to use\n",
                "TARGET_ACCEPT = 0.9  # Target acceptance rate for NUTS sampler\n",
                "\n",
                "# Data selection parameters\n",
                "camera_name = \"PPCX_Tele\"\n",
                "reference_date = \"2017-07-08\"\n",
                "days_before_to_include = 0  # Number of days before reference date to include\n",
                "days_after_to_include = 0  # Number of days after reference date to include\n",
                "dt_min = 72  # Minimum time difference between images in hours\n",
                "dt_max = 96  # Maximum time difference between images in hours\n",
                "reference_start_date = datetime.strptime(reference_date, \"%Y-%m-%d\") - pd.Timedelta(\n",
                "    days=days_before_to_include\n",
                ")\n",
                "reference_end_date = datetime.strptime(reference_date, \"%Y-%m-%d\") + pd.Timedelta(\n",
                "    days=days_after_to_include\n",
                ")\n",
                "\n",
                "# Subsampling and filtering parameters\n",
                "variables_names = [\"V\"]\n",
                "SUBSAMPLE_FACTOR = 1  # 1=Take every n point\n",
                "SUBSAMPLE_METHOD = \"random\"  # or 'random', 'stratified'\n",
                "filter_kwargs = dict(\n",
                "    filter_outliers=False,\n",
                "    tails_percentile=0.005,\n",
                "    min_velocity=0.5,\n",
                "    apply_2d_median=False,\n",
                "    median_window_size=5,\n",
                "    median_threshold_factor=3,\n",
                "    apply_2d_gaussian=False,\n",
                "    gaussian_sigma=1.0,\n",
                ")\n",
                "\n",
                "# == PRIORS and ROI ==\n",
                "# Define a specific prior probability for each sector (overrides PRIOR_STRENGTH)\n",
                "# This is a dictionary where keys are sector names and values are lists of prior probabilities (Sector names must match those in the XML file)\n",
                "# Sector name: [P(Cluster A), P(Cluster B), P(Cluster C)...]\n",
                "# PRIOR_PROBABILITY = {\n",
                "#     \"A\": [1.0, 0.0, 0.0],\n",
                "#     \"B\": [0.1, 0.7, 0.2],\n",
                "#     \"C\": [0.0, 0.2, 0.8],\n",
                "# }\n",
                "SECTOR_PRIOR_FILE = Path(\"data/priors_4_sectors.xml\")\n",
                "PRIOR_PROBABILITY = {\n",
                "    \"A\": [1.0, 0.0, 0.0, 0.0],\n",
                "    \"B\": [0.1, 0.7, 0.2, 0.0],\n",
                "    \"C\": [0.0, 0.2, 0.8, 0.0],\n",
                "    \"D\": [0.0, 0.0, 0.3, 0.7],\n",
                "}\n",
                "\n",
                "roi_path = Path(\"data/roi.xml\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "ad4b243d",
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'read_polygons_from_cvat' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Read roi and spatial priors\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m roi = \u001b[43mread_polygons_from_cvat\u001b[49m(roi_path, image_name=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m      3\u001b[39m sectors = read_polygons_from_cvat(SECTOR_PRIOR_FILE, image_name=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Check that at least the reference date or an interval of dates is provided\u001b[39;00m\n",
                        "\u001b[31mNameError\u001b[39m: name 'read_polygons_from_cvat' is not defined"
                    ]
                }
            ],
            "source": [
                "# Read roi and spatial priors\n",
                "roi = read_polygons_from_cvat(roi_path, image_name=None)\n",
                "sectors = read_polygons_from_cvat(SECTOR_PRIOR_FILE, image_name=None)\n",
                "\n",
                "# Check that at least the reference date or an interval of dates is provided\n",
                "if not (reference_date or (reference_start_date and reference_end_date)):\n",
                "    raise ValueError(\n",
                "        \"Either reference_date or both reference_start_date and reference_end_date must be provided.\"\n",
                "    )\n",
                "\n",
                "# Fetch DIC ids\n",
                "dic_ids = fetch_dic_analysis_ids(\n",
                "    db_engine,\n",
                "    camera_name=camera_name,\n",
                "    reference_date=reference_date,\n",
                "    reference_date_start=reference_start_date,\n",
                "    reference_date_end=reference_end_date,\n",
                "    dt_hours_min=dt_min,\n",
                "    dt_hours_max=dt_max,\n",
                ")\n",
                "if len(dic_ids) < 1:\n",
                "    raise ValueError(\"No DIC analyses found for the given criteria\")\n",
                "\n",
                "# Get DIC analysis metadata\n",
                "dic_analyses = get_dic_analysis_by_ids(db_engine=db_engine, dic_ids=dic_ids)\n",
                "logger.info(\"Fetched DIC analysis:\")\n",
                "for _, row in dic_analyses.iterrows():\n",
                "    print(\n",
                "        f\"DIC ID: {row['dic_id']}, date: {row['reference_date']}, dt (hrs): {row['dt_hours']}, Master: {row['master_timestamp']}, Slave: {row['slave_timestamp']}\"\n",
                "    )\n",
                "print(\"Summary of selected DIC analyses:\")\n",
                "print(dic_analyses.describe())\n",
                "\n",
                "\n",
                "# Output paths\n",
                "date_start = dic_analyses.iloc[0][\"master_timestamp\"].strftime(\"%Y-%m-%d\")\n",
                "date_end = dic_analyses.iloc[0][\"slave_timestamp\"].strftime(\"%Y-%m-%d\")\n",
                "output_dir = Path(\"output\") / f\"{camera_name}_{date_end}_mcmc_multiscale\"\n",
                "output_dir.mkdir(parents=True, exist_ok=True)\n",
                "base_name = f\"{date_start}_{date_end}\"\n",
                "\n",
                "# Get master image\n",
                "master_image_id = dic_analyses[\"master_image_id\"].iloc[0]\n",
                "img = get_image(image_id=master_image_id, config=config)\n",
                "\n",
                "# Fetch DIC data\n",
                "out = get_multi_dic_data(\n",
                "    dic_ids,\n",
                "    stack_results=False,\n",
                "    config=config,\n",
                ")\n",
                "logger.info(f\"Found stack of {len(out)} DIC dataframes. Run filtering...\")\n",
                "\n",
                "# Apply filter for each df in the dictionary and then stack them\n",
                "processed = []\n",
                "for src_id, df_src in out.items():\n",
                "    try:\n",
                "        # Filter only points inside the spatial priors sectors\n",
                "        df_src = filter_dataframe_by_polygons(df_src, polygons=roi)\n",
                "\n",
                "        # Apply other DIC filters if any\n",
                "        df_src = apply_dic_filters(df_src, **filter_kwargs)\n",
                "\n",
                "        # Append processed dataframe to the list\n",
                "        processed.append(df_src)\n",
                "    except Exception as exc:\n",
                "        logger.warning(\"Filtering failed for %s: %s\", src_id, exc)\n",
                "if not processed:\n",
                "    raise RuntimeError(\"No dataframes left after filtering.\")\n",
                "# Stack all processed dataframes\n",
                "df = pd.concat(processed, ignore_index=True)\n",
                "logger.info(\"Data shape after filtering and stacking: %s\", df.shape)\n",
                "\n",
                "# Apply subsampling\n",
                "if SUBSAMPLE_FACTOR > 1:\n",
                "    df_subsampled = spatial_subsample(\n",
                "        df, n_subsample=SUBSAMPLE_FACTOR, method=SUBSAMPLE_METHOD\n",
                "    )\n",
                "    df = df_subsampled\n",
                "    logger.info(f\"Data shape after subsampling: {df.shape}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "02c71629",
            "metadata": {},
            "source": [
                "## RUN MCMC multiple times with different smoothing scales and median the clustering results\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fc24fe9c",
            "metadata": {},
            "outputs": [],
            "source": [
                "## Helper function to preprocess velocity features\n",
                "\n",
                "\n",
                "def run_mcmc_clustering(\n",
                "    df_input,\n",
                "    prior_probs,\n",
                "    sectors,\n",
                "    output_dir,\n",
                "    base_name,\n",
                "    img=None,\n",
                "    variables_names=None,\n",
                "    transform_velocity=\"none\",\n",
                "    transform_params=None,\n",
                "    mu_params=None,\n",
                "    sigma_params=None,\n",
                "    feature_weights=None,\n",
                "    sample_args=None,\n",
                "    mrf_regularization: bool = False,\n",
                "    mrf_kwargs: dict | None = None,\n",
                "    second_pass: str = \"full\",  # \"skip\" | \"short\" | \"full\"\n",
                "    second_pass_sample_args: dict | None = None,\n",
                "    random_seed=8927,\n",
                "):\n",
                "    \"\"\"\n",
                "    Run MCMC-based clustering on velocity data with flexible velocity transformations.\n",
                "\n",
                "    Parameters:\n",
                "    -----------\n",
                "    df_input : pandas.DataFrame\n",
                "        Input dataframe with 'x', 'y', 'V' columns\n",
                "    transform_velocity : str, default=\"none\"\n",
                "        Type of velocity transformation: \"power\", \"exponential\", \"threshold\", \"sigmoid\", or \"none\"\n",
                "    transform_params : dict, optional\n",
                "        Parameters for velocity transformation (see preprocess_velocity_features for details)\n",
                "    \"\"\"\n",
                "\n",
                "    # --- helper: build initvals from idata posterior means (warm-start) ---\n",
                "    def _initvals_from_idata(idata_in, n_chains):\n",
                "        mu_mean = idata_in.posterior[\"mu\"].mean(dim=[\"chain\", \"draw\"]).values\n",
                "        sigma_mean = idata_in.posterior[\"sigma\"].mean(dim=[\"chain\", \"draw\"]).values\n",
                "        # Ensure shapes match the model dims; return a list of per-chain dicts\n",
                "        init = {\"mu\": mu_mean, \"sigma\": sigma_mean}\n",
                "        return [init for _ in range(n_chains)]\n",
                "\n",
                "    logger.info(f\"Running MCMC clustering for {base_name}...\")\n",
                "\n",
                "    # Default parameters if not provided\n",
                "    if mu_params is None:\n",
                "        mu_params = {\"mu\": 0, \"sigma\": 1}\n",
                "    if sigma_params is None:\n",
                "        sigma_params = {\"sigma\": 1}\n",
                "    if sample_args is None:\n",
                "        sample_args = dict(\n",
                "            target_accept=0.95,\n",
                "            draws=2000,\n",
                "            tune=1000,\n",
                "            chains=4,\n",
                "            cores=4,\n",
                "            random_seed=random_seed,\n",
                "        )\n",
                "    if variables_names is None:\n",
                "        variables_names = [\"V\"]\n",
                "\n",
                "    if \"V\" not in df_input.columns:\n",
                "        raise ValueError(\"Input dataframe must contain 'V' column for velocities.\")\n",
                "\n",
                "    # Preprocess velocity features to enhance high velocities\n",
                "    velocities, transform_info = preprocess_velocity_features(\n",
                "        velocities=df_input[\"V\"].to_numpy(),\n",
                "        velocity_transform=transform_velocity,\n",
                "        velocity_params=transform_params,\n",
                "    )\n",
                "\n",
                "    # Extract data array for clustering\n",
                "    if len(variables_names) > 1:\n",
                "        # Concatenate other features to velocities\n",
                "        additional_vars = variables_names.copy()\n",
                "        if \"V\" in additional_vars:\n",
                "            additional_vars.remove(\"V\")\n",
                "        additional_data = df_input[additional_vars].to_numpy()\n",
                "        data_array = np.column_stack((velocities, additional_data))\n",
                "    else:\n",
                "        # Use only velocities\n",
                "        data_array = velocities.reshape(-1, 1)\n",
                "\n",
                "    # Scale data for model input\n",
                "    scaler = StandardScaler()\n",
                "    scaler.fit(data_array)\n",
                "    joblib.dump(scaler, output_dir / f\"{base_name}_scaler.joblib\")\n",
                "    data_array_scaled = scaler.transform(data_array)\n",
                "\n",
                "    # Build model\n",
                "    logger.info(f\"Running MCMC clustering for {base_name}...\")\n",
                "    model = mcmc.build_marginalized_mixture_model(\n",
                "        data_array_scaled,\n",
                "        prior_probs,\n",
                "        sectors,\n",
                "        mu_params=mu_params,\n",
                "        sigma_params=sigma_params,\n",
                "        feature_weights=feature_weights,\n",
                "    )\n",
                "\n",
                "    # Sample model (1st pass)\n",
                "    idata, convergence_flag = mcmc.sample_model(\n",
                "        model, output_dir, base_name, **sample_args\n",
                "    )\n",
                "    if not convergence_flag:\n",
                "        idata_summary = az.summary(idata, var_names=[\"mu\", \"sigma\"])\n",
                "        logger.info(f\"MCMC did not converge. Summary:\\n{idata_summary}\")\n",
                "\n",
                "    # --- MRF regularization of priors and optional re-sample ---\n",
                "    prior_used = prior_probs\n",
                "    if mrf_regularization:\n",
                "        x_pos = df_input[\"x\"].to_numpy()\n",
                "        y_pos = df_input[\"y\"].to_numpy()\n",
                "        mkw = dict(n_neighbors=8, length_scale=50, beta=2.0, n_iter=5)\n",
                "        if mrf_kwargs:\n",
                "            mkw.update(mrf_kwargs)\n",
                "        prior_mrf, q_mrf = mcmc.mrf_regularization(\n",
                "            data_array_scaled, idata, prior_probs, x_pos, y_pos, **mkw\n",
                "        )\n",
                "        prior_used = prior_mrf\n",
                "\n",
                "        # visualize refined priors\n",
                "        try:\n",
                "            fig, _ = mcmc.plot_spatial_priors(df_input, prior_mrf, img=img)\n",
                "            fig.savefig(\n",
                "                output_dir / f\"{base_name}_mrf_priors.png\", dpi=150, bbox_inches=\"tight\"\n",
                "            )\n",
                "            plt.close(fig)\n",
                "        except Exception as exc:\n",
                "            logger.warning(f\"Could not plot MRF priors: {exc}\")\n",
                "\n",
                "    # Decide second pass strategy\n",
                "    if mrf_regularization and second_pass.lower() == \"skip\":\n",
                "        # Fastest: don't re-sample. Use q_mrf as final posterior_probs and argmax as labels.\n",
                "        posterior_probs = q_mrf\n",
                "        cluster_pred = np.argmax(posterior_probs, axis=1)\n",
                "        uncertainty = 1.0 - posterior_probs.max(axis=1)\n",
                "        # keep idata from 1st pass for plots/params\n",
                "    else:\n",
                "        # Re-sample with refined priors (short or full)\n",
                "        if mrf_regularization:\n",
                "            with model:\n",
                "                pm.set_data({\"prior_w\": prior_used})\n",
                "\n",
                "        # Allow short second pass and warm start\n",
                "        sp2_args = dict(**sample_args)\n",
                "        if second_pass.lower() == \"short\":\n",
                "            # much fewer draws/tune; fewer chains can also help\n",
                "            sp2_args.update(dict(draws=600, tune=400, chains=2, cores=2))\n",
                "            if second_pass_sample_args:\n",
                "                sp2_args.update(second_pass_sample_args)\n",
                "        elif second_pass_sample_args:\n",
                "            sp2_args.update(second_pass_sample_args)\n",
                "\n",
                "        # Warm-start from previous posterior means\n",
                "        initvals = _initvals_from_idata(idata, sp2_args.get(\"chains\", 2))\n",
                "\n",
                "        with model:\n",
                "            # pass initvals through sample_model if it supports, else call pm.sample directly\n",
                "            try:\n",
                "                idata, convergence_flag = mcmc.sample_model(\n",
                "                    model,\n",
                "                    output_dir,\n",
                "                    base_name + (\"_mrf\" if mrf_regularization else \"\"),\n",
                "                    initvals=initvals,\n",
                "                    **sp2_args,\n",
                "                )\n",
                "            except TypeError:\n",
                "                # fallback if your wrapper doesn't accept initvals\n",
                "                idata = pm.sample(**sp2_args)\n",
                "                convergence_flag = True\n",
                "\n",
                "        # Compute posterior-based assignments\n",
                "        posterior_probs, cluster_pred, uncertainty = mcmc.compute_posterior_assignments(\n",
                "            idata, n_posterior_samples=200\n",
                "        )\n",
                "\n",
                "    # Generate plots\n",
                "    fig = mcmc.plot_velocity_clustering(\n",
                "        df_features=df_input,\n",
                "        img=img,\n",
                "        idata=idata,\n",
                "        cluster_pred=cluster_pred,\n",
                "        posterior_probs=posterior_probs,\n",
                "        scaler=scaler,\n",
                "    )\n",
                "    fig.savefig(\n",
                "        output_dir / f\"{base_name}_results.png\",\n",
                "        dpi=300,\n",
                "        bbox_inches=\"tight\",\n",
                "    )\n",
                "    plt.close(fig)\n",
                "\n",
                "    # Trace plots\n",
                "    fig, axes = plt.subplots(2, 2, figsize=(10, 6))\n",
                "    az.plot_trace(\n",
                "        idata, var_names=[\"mu\", \"sigma\"], axes=axes, compact=True, legend=True\n",
                "    )\n",
                "    fig.savefig(output_dir / f\"{base_name}_trace_plots.png\", dpi=150)\n",
                "    plt.close(fig)\n",
                "\n",
                "    # Forest plots\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
                "    az.plot_forest(idata, var_names=[\"mu\", \"sigma\"], combined=True, ess=True, ax=axes)\n",
                "    fig.savefig(output_dir / f\"{base_name}_forest_plot.png\", dpi=150)\n",
                "    plt.close(fig)\n",
                "\n",
                "    # Collect and save metadata\n",
                "    metadata = mcmc.collect_run_metadata(\n",
                "        idata=idata,\n",
                "        convergence_flag=convergence_flag,\n",
                "        data_array_scaled=data_array_scaled,\n",
                "        variables_names=variables_names,\n",
                "        sectors=sectors,\n",
                "        prior_probs=prior_probs,\n",
                "        sample_args=sample_args,\n",
                "        frame=locals(),\n",
                "    )\n",
                "    mcmc.save_run_metadata(output_dir, base_name, metadata)\n",
                "\n",
                "    # Return results dictionary\n",
                "    result = {\n",
                "        \"metadata\": metadata,\n",
                "        \"idata\": idata,\n",
                "        \"scaler\": scaler,\n",
                "        \"convergence_flag\": convergence_flag,\n",
                "        \"posterior_probs\": posterior_probs,\n",
                "        \"cluster_pred\": cluster_pred,\n",
                "        \"uncertainty\": uncertainty,\n",
                "    }\n",
                "\n",
                "    plt.close(\"all\")\n",
                "    return result"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e70ca00f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Assign spatial priors\n",
                "prior_probs = mcmc.assign_spatial_priors(\n",
                "    x=df[\"x\"].to_numpy(),\n",
                "    y=df[\"y\"].to_numpy(),\n",
                "    polygons=sectors,\n",
                "    prior_probs=PRIOR_PROBABILITY,\n",
                "    method=\"exponential\",\n",
                "    method_kws={\"decay_rate\": 0.001},\n",
                ")\n",
                "\n",
                "fig, axes = mcmc.plot_spatial_priors(df, prior_probs, img=img)\n",
                "fig.savefig(\n",
                "    output_dir / f\"{base_name}_spatial_priors.jpg\",\n",
                "    dpi=150,\n",
                "    bbox_inches=\"tight\",\n",
                ")\n",
                "plt.close(fig)\n",
                "\n",
                "# Plot velocity field\n",
                "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
                "ax.set_title(\"Velocity Field\", fontsize=14, pad=10)\n",
                "ax.imshow(img, alpha=0.5, cmap=\"gray\")\n",
                "magnitudes = df[\"V\"].to_numpy()\n",
                "vmin = 0.0\n",
                "vmax = np.max(magnitudes)\n",
                "norm = Normalize(vmin=vmin, vmax=vmax)\n",
                "q = ax.quiver(\n",
                "    df[\"x\"].to_numpy(),\n",
                "    df[\"y\"].to_numpy(),\n",
                "    df[\"u\"].to_numpy(),\n",
                "    df[\"v\"].to_numpy(),\n",
                "    magnitudes,\n",
                "    scale=None,\n",
                "    scale_units=\"xy\",\n",
                "    angles=\"xy\",\n",
                "    cmap=\"viridis\",\n",
                "    norm=norm,\n",
                "    width=0.008,\n",
                "    headwidth=2.5,\n",
                "    alpha=1.0,\n",
                ")\n",
                "cbar = fig.colorbar(q, ax=ax, shrink=0.8, aspect=20, pad=0.02)\n",
                "cbar.set_label(\"Velocity Magnitude\", rotation=270, labelpad=15)\n",
                "ax.set_aspect(\"equal\")\n",
                "ax.set_xticks([])\n",
                "ax.set_yticks([])\n",
                "ax.grid(False)\n",
                "\n",
                "fig.savefig(\n",
                "    output_dir / f\"{base_name}_velocity_field.jpg\",\n",
                "    dpi=150,\n",
                "    bbox_inches=\"tight\",\n",
                ")\n",
                "plt.close(fig)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "670eada2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define sigma values for Gaussian smoothing\n",
                "sigma_values = [2]\n",
                "variables_names = [\"V\"]\n",
                "\n",
                "\n",
                "# Loop through smoothing scales\n",
                "results = []\n",
                "for sigma in sigma_values:\n",
                "    logger.info(f\"Processing with Gaussian smoothing sigma={sigma}...\")\n",
                "\n",
                "    # Create scale-specific base name\n",
                "    scale_base_name = f\"{date_start}_{date_end}_sigma{sigma}\"\n",
                "\n",
                "    # Apply Gaussian smoothing if needed (skipped for sigma=0)\n",
                "    df_run = apply_2d_gaussian_filter(df, sigma=sigma)\n",
                "\n",
                "    # Adjust model parameters based on scale\n",
                "    mu_params = {\"mu\": 0, \"sigma\": 1 if sigma <= 2 else 0.5}\n",
                "    sigma_params = {\"sigma\": 1 if sigma <= 2 else 0.5}\n",
                "\n",
                "    # Run MCMC clustering with the smoothed data\n",
                "    result = run_mcmc_clustering(\n",
                "        df_input=df_run,\n",
                "        prior_probs=prior_probs,\n",
                "        sectors=sectors,\n",
                "        variables_names=variables_names,\n",
                "        output_dir=output_dir,\n",
                "        base_name=scale_base_name,\n",
                "        img=img,\n",
                "        # transform_velocity=\"sigmoid\",\n",
                "        # transform_params={\"midpoint_percentile\": 70, \"steepness\": 2.0},\n",
                "        mu_params=mu_params,\n",
                "        sigma_params=sigma_params,\n",
                "        random_seed=RANDOM_SEED,\n",
                "        mrf_regularization=True,\n",
                "        mrf_kwargs=dict(n_neighbors=8, length_scale=50, beta=2.0, n_iter=5),\n",
                "        # Speed choices:\n",
                "        # 1) \"short\": short sampling + warm-start (recommended default):\n",
                "        # 2) \"skip\": fastest, rely only on MRF priors, no re-sampling\n",
                "        second_pass=\"short\",\n",
                "        second_pass_sample_args=dict(\n",
                "            draws=500, tune=300, chains=4, cores=4, target_accept=0.9\n",
                "        ),\n",
                "    )\n",
                "\n",
                "    # Add scale information to result\n",
                "    result[\"sigma\"] = sigma\n",
                "\n",
                "    # Append to results list\n",
                "    results.append(result)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "507da71e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===  If a multi-scale approach was used, aggregate the results.\n",
                "if len(sigma_values) > 1:\n",
                "    aggregated_results = aggregate_multiscale_clustering(\n",
                "        results,\n",
                "        similarity_threshold=0.7,\n",
                "        overall_threshold=0.8,\n",
                "        fig_path=output_dir\n",
                "        / f\"{reference_start_date}_{reference_end_date}_similarity_heatmap.jpg\",\n",
                "    )\n",
                "\n",
                "    # Unpack aggregated results\n",
                "    cluster_pred = aggregated_results[\"combined_cluster_pred\"]\n",
                "    posterior_probs = aggregated_results[\"avg_posterior_probs\"]\n",
                "    entropy = aggregated_results[\"avg_entropy\"]\n",
                "    similarity_matrix = aggregated_results[\"similarity_matrix\"]\n",
                "    stability_score = aggregated_results[\"stability_score\"]\n",
                "    valid_scales = aggregated_results[\"valid_scales\"]\n",
                "\n",
                "else:\n",
                "    # Otherwise extract the single result\n",
                "    cluster_pred = results[0][\"cluster_pred\"]\n",
                "    posterior_probs = results[0][\"posterior_probs\"]\n",
                "    entropy = -np.sum(posterior_probs * np.log(posterior_probs + 1e-10), axis=1)\n",
                "    similarity_matrix = None\n",
                "    stability_score = None\n",
                "    valid_scales = None\n",
                "\n",
                "\n",
                "# ===  Save final clustering results\n",
                "cluster_aggregation_outs = {\n",
                "    \"cluster_pred\": cluster_pred,\n",
                "    \"posterior_probs\": posterior_probs,\n",
                "    \"entropy\": entropy,\n",
                "    \"similarity_matrix\": similarity_matrix,\n",
                "    \"stability_score\": stability_score,\n",
                "    \"valid_scales\": valid_scales,\n",
                "}\n",
                "joblib.dump(\n",
                "    cluster_aggregation_outs,\n",
                "    output_dir\n",
                "    / f\"{reference_start_date}_{reference_end_date}_kinematic_clustering_results.joblib\",\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "16f52f47",
            "metadata": {},
            "source": [
                "## Morpho-kinematic analysis\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fe487b08",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Retrieve data\n",
                "df_smooth = apply_2d_gaussian_filter(df, sigma=1)\n",
                "x = df_smooth[\"x\"].to_numpy()\n",
                "y = df_smooth[\"y\"].to_numpy()\n",
                "v = df_smooth[\"V\"].to_numpy()\n",
                "kin_cluster = np.asarray(cluster_pred.copy())\n",
                "\n",
                "X, Y, kin_cluster_grid = create_2d_grid(x=x, y=y, labels=kin_cluster)\n",
                "\n",
                "# Filter out small clusters\n",
                "kin_cluster_grid = remove_small_grid_components(\n",
                "    kin_cluster_grid, min_size=100, connectivity=8\n",
                ")\n",
                "\n",
                "# Split clusters along detected discontinuities\n",
                "kin_cluster_grid, split_mapping = split_disconnected_components(\n",
                "    kin_cluster_grid, connectivity=8, start_label=0\n",
                ")\n",
                "kin_cluster, x, y = map_grid_to_points(X, Y, kin_cluster_grid, x, y, keep_nan=True)\n",
                "\n",
                "# Remove non classified points (-1 label)\n",
                "valid_mask = kin_cluster >= 0\n",
                "x = x[valid_mask]\n",
                "y = y[valid_mask]\n",
                "v = v[valid_mask]\n",
                "kin_cluster = kin_cluster[valid_mask]\n",
                "\n",
                "# Order clusters by median y descending (bottom = largest y first)\n",
                "clusters_ids = np.unique(kin_cluster)\n",
                "cluster_median_y = {int(c): float(np.median(y[kin_cluster == c])) for c in clusters_ids}\n",
                "ordered_clusters_ids = sorted(\n",
                "    clusters_ids, key=lambda c: cluster_median_y[int(c)], reverse=True\n",
                ")\n",
                "\n",
                "# === Compute similarity scores with prior clusters\n",
                "# Create a \"prior class\" assignment based on the sector with highest probability\n",
                "sector_names = list(PRIOR_PROBABILITY.keys())\n",
                "sector_assignments = np.zeros_like(kin_cluster)\n",
                "for i, point_probs in enumerate(prior_probs):\n",
                "    sector_assignments[i] = np.argmax(point_probs)\n",
                "\n",
                "# Compute similarity metrics\n",
                "ari = adjusted_rand_score(sector_assignments, kin_cluster)\n",
                "ami = adjusted_mutual_info_score(sector_assignments, kin_cluster)\n",
                "\n",
                "# === Make final clustering plot after cleaning\n",
                "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
                "ax.imshow(img, alpha=0.5, cmap=\"gray\")\n",
                "colormap = plt.get_cmap(\"tab10\")\n",
                "for i, label in enumerate(clusters_ids):\n",
                "    mask = kin_cluster == label\n",
                "    ax.scatter(\n",
                "        x[mask],\n",
                "        y[mask],\n",
                "        color=colormap(i),\n",
                "        label=f\"Cluster {label}\",\n",
                "        s=10,\n",
                "        alpha=0.7,\n",
                "    )\n",
                "ax.legend(loc=\"upper right\", framealpha=0.9, fontsize=10)\n",
                "ax.set_aspect(\"equal\")\n",
                "\n",
                "if valid_scales is not None and len(valid_scales) > 1:\n",
                "    title = f\"Combined Clustering (scales: {valid_scales}, stability: {stability_score if stability_score is not None else '':.2f})\\nPrior Agreement: AMI={ami if ami is not None else 0.0:.2f}\"\n",
                "else:\n",
                "    title = f\"Clustering (scale: {sigma_values[0]})\\nPrior Agreement: AMI={ami if ami is not None else 0.0:.2f}\"\n",
                "\n",
                "ax.set_title(title)\n",
                "plt.savefig(\n",
                "    output_dir\n",
                "    / f\"{reference_start_date}_{reference_end_date}_kinematic_clustering.png\",\n",
                "    dpi=300,\n",
                "    bbox_inches=\"tight\",\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3c900126",
            "metadata": {},
            "outputs": [],
            "source": [
                "MINOR_OVERLAP_THRESHOLD = 0.9\n",
                "base_colors = {\n",
                "    \"A\": \"#b3140b\",\n",
                "    \"B\": \"#ee9c21\",\n",
                "    \"C\": \"#f1ee30\",\n",
                "    \"D\": \"#5fb61c\",\n",
                "}\n",
                "cmap = plt.get_cmap(\"tab20\")\n",
                "\n",
                "assignment = auto_assign_mk_sectors(\n",
                "    x=x,\n",
                "    y=y,\n",
                "    kin_cluster=kin_cluster,\n",
                "    ordered_clusters_ids=ordered_clusters_ids,\n",
                "    overlap_threshold=MINOR_OVERLAP_THRESHOLD,\n",
                ")\n",
                "\n",
                "mk_label_str = assignment[\"mk_label_str\"]\n",
                "mk_label_id = assignment[\"mk_label_id\"]\n",
                "major_clusters = assignment[\"major_clusters\"]\n",
                "major_label_map = assignment[\"major_label_map\"]\n",
                "minor_parent = assignment[\"minor_parent\"]\n",
                "cluster_to_label = assignment[\"cluster_to_label\"]\n",
                "polygons_major = assignment[\"polygons_major\"]\n",
                "polygons_minor = assignment[\"polygons_minor\"]\n",
                "\n",
                "label_order = sorted(assignment[\"label_to_index\"].items(), key=lambda kv: kv[1])\n",
                "unique_mk = np.array([lab for lab, _ in label_order], dtype=object)\n",
                "counts = np.array([np.sum(mk_label_str == lab) for lab in unique_mk], dtype=int)\n",
                "\n",
                "logger.info(\"Major sectors: %s\", major_label_map)\n",
                "if minor_parent:\n",
                "    logger.info(\n",
                "        \"Minor cluster mapping: %s\",\n",
                "        {\n",
                "            cluster_id: {\n",
                "                \"label\": cluster_to_label[cluster_id],\n",
                "                \"parent\": cluster_to_label[parent_id],\n",
                "                \"overlap\": f\"{overlap_ratio:.2f}\",\n",
                "            }\n",
                "            for cluster_id, (parent_id, overlap_ratio) in minor_parent.items()\n",
                "        },\n",
                "    )\n",
                "else:\n",
                "    logger.info(\"No minor clusters detected.\")\n",
                "\n",
                "\n",
                "print(\"Morpho-kinematic assignment summary:\")\n",
                "for label, count in zip(unique_mk, counts, strict=False):\n",
                "    print(f\"  {label}: {count} points\")\n",
                "\n",
                "\n",
                "colors = {}\n",
                "for idx, label in enumerate(unique_mk):\n",
                "    major_key = \"\".join(filter(str.isalpha, label)) or label\n",
                "    color = base_colors.get(label) or base_colors.get(major_key)\n",
                "    if color is None:\n",
                "        color = mcolors.to_hex(cmap(idx % cmap.N))\n",
                "    colors[label] = color\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(8, 8))\n",
                "ax.imshow(img, alpha=0.5, cmap=\"gray\")\n",
                "\n",
                "major_labels = [\n",
                "    cluster_to_label[cid] for cid in major_clusters if cid in cluster_to_label\n",
                "]\n",
                "for label in major_labels:\n",
                "    poly = polygons_major.get(label)\n",
                "    if poly is not None:\n",
                "        draw_polygon(\n",
                "            ax, poly, label, colors.get(label, \"#444444\"), fill_alpha=0.12, zorder=1\n",
                "        )\n",
                "\n",
                "minor_labels = [label for label in unique_mk if label not in major_labels]\n",
                "for label in minor_labels:\n",
                "    poly = polygons_minor.get(label)\n",
                "    if poly is not None:\n",
                "        draw_polygon(\n",
                "            ax, poly, label, colors.get(label, \"#888888\"), fill_alpha=0.08, zorder=2\n",
                "        )\n",
                "\n",
                "handles, labels = ax.get_legend_handles_labels()\n",
                "unique_handles = []\n",
                "unique_labels = []\n",
                "seen = set()\n",
                "for handle, name in zip(handles, labels, strict=False):\n",
                "    if name in seen:\n",
                "        continue\n",
                "    seen.add(name)\n",
                "    unique_handles.append(handle)\n",
                "    unique_labels.append(name)\n",
                "if unique_handles:\n",
                "    ax.legend(\n",
                "        unique_handles,\n",
                "        unique_labels,\n",
                "        loc=\"upper right\",\n",
                "        fontsize=9,\n",
                "        framealpha=0.9,\n",
                "    )\n",
                "\n",
                "ax.set_title(\"Morpho-Kinematic Sectors\")\n",
                "ax.set_aspect(\"equal\")\n",
                "fig.savefig(\n",
                "    output_dir\n",
                "    / f\"{reference_start_date}_{reference_end_date}_mk_sectors_perimeters.png\",\n",
                "    dpi=300,\n",
                "    bbox_inches=\"tight\",\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2e14f292",
            "metadata": {},
            "outputs": [],
            "source": [
                "mk_stats = compute_mk_sector_stats(\n",
                "    polygons_major,\n",
                "    mk_label_str,\n",
                "    x=x,\n",
                "    y=y,\n",
                "    v=v,\n",
                "    img_shape=np.asarray(img).shape if img is not None else None,\n",
                "    rasterize=True,\n",
                ")\n",
                "mk_stats.to_csv(\n",
                "    output_dir / f\"{reference_start_date}_{reference_end_date}_mk_sector_stats.csv\",\n",
                "    index=False,\n",
                ")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ppcx",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
